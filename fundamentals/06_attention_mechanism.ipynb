{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanisms: The Foundation of Transformers\n",
    "\n",
    "Attention is the breakthrough that powers modern AI systems like GPT, BERT, and image generation models. It allows networks to focus on relevant parts of the input.\n",
    "\n",
    "## What You'll Learn:\n",
    "1. **The Attention Problem** - Why we need attention\n",
    "2. **Attention Basics** - Query, Key, Value\n",
    "3. **Scaled Dot-Product Attention** - The core mechanism\n",
    "4. **Self-Attention** - Attending to your own sequence\n",
    "5. **Multi-Head Attention** - Multiple attention patterns\n",
    "6. **Implementation** - From scratch in NumPy and PyTorch\n",
    "\n",
    "**Key Idea:** Attention lets the model decide which parts of the input to focus on for each output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Attention Problem\n",
    "\n",
    "### Traditional RNNs/LSTMs:\n",
    "Process sequences sequentially and compress everything into a fixed-size hidden state.\n",
    "\n",
    "**Problems:**\n",
    "1. **Information bottleneck**: Long sequences lose information\n",
    "2. **No parallelization**: Must process sequentially\n",
    "3. **Fixed context**: Can't focus on specific parts\n",
    "\n",
    "### Example: Translation\n",
    "**English:** \"The cat sat on the mat\"\n",
    "\n",
    "**French:** \"Le chat s'est assis sur le tapis\"\n",
    "\n",
    "When translating \"chat\", we should focus on \"cat\" in the input, not \"mat\"!\n",
    "\n",
    "**Attention** solves this by letting the model dynamically focus on relevant input positions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Attention Basics: Query, Key, Value\n",
    "\n",
    "Attention is like a **lookup mechanism** similar to a database:\n",
    "\n",
    "- **Query (Q)**: \"What am I looking for?\"\n",
    "- **Key (K)**: \"What does each item represent?\"\n",
    "- **Value (V)**: \"What is the actual content?\"\n",
    "\n",
    "### Real-World Analogy:\n",
    "Imagine searching your memory for \"that Italian restaurant\":\n",
    "- **Query**: \"Italian restaurant I liked\"\n",
    "- **Keys**: Properties of all restaurants you know\n",
    "- **Values**: Full information about each restaurant\n",
    "\n",
    "Your brain compares the query against keys to find matching restaurants, then retrieves their values.\n",
    "\n",
    "### Mathematical Formulation:\n",
    "\n",
    "**Attention(Q, K, V) = weighted sum of Values**\n",
    "\n",
    "Where weights depend on how well Query matches each Key."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Scaled Dot-Product Attention\n",
    "\n",
    "The most common attention mechanism.\n",
    "\n",
    "### Formula:\n",
    "\n",
    "$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$\n",
    "\n",
    "### Steps:\n",
    "\n",
    "1. **Compute scores**: $S = QK^T$ (how well does Q match each K?)\n",
    "2. **Scale**: $S_{scaled} = \\frac{S}{\\sqrt{d_k}}$ (prevent large values)\n",
    "3. **Normalize**: $A = \\text{softmax}(S_{scaled})$ (get probabilities)\n",
    "4. **Weighted sum**: $\\text{Output} = AV$ (combine values)\n",
    "\n",
    "### Why scale by $\\sqrt{d_k}$?\n",
    "When $d_k$ is large, dot products grow large in magnitude, pushing softmax into regions with small gradients. Scaling prevents this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(Q, K, V, mask=None):\n",
    "    \"\"\"\n",
    "    Compute scaled dot-product attention\n",
    "    \n",
    "    Args:\n",
    "        Q: Query matrix (seq_len_q, d_k)\n",
    "        K: Key matrix (seq_len_k, d_k)\n",
    "        V: Value matrix (seq_len_v, d_v) where seq_len_v == seq_len_k\n",
    "        mask: Optional mask to ignore certain positions\n",
    "    \n",
    "    Returns:\n",
    "        output: Attention output (seq_len_q, d_v)\n",
    "        attention_weights: Attention distribution (seq_len_q, seq_len_k)\n",
    "    \"\"\"\n",
    "    d_k = Q.shape[-1]\n",
    "    \n",
    "    # 1. Compute attention scores\n",
    "    scores = np.matmul(Q, K.T)  # (seq_len_q, seq_len_k)\n",
    "    \n",
    "    # 2. Scale\n",
    "    scores = scores / np.sqrt(d_k)\n",
    "    \n",
    "    # 3. Apply mask (if provided)\n",
    "    if mask is not None:\n",
    "        scores = scores + (mask * -1e9)  # add large negative to masked positions\n",
    "    \n",
    "    # 4. Softmax to get attention weights\n",
    "    attention_weights = np.exp(scores - np.max(scores, axis=-1, keepdims=True))\n",
    "    attention_weights = attention_weights / np.sum(attention_weights, axis=-1, keepdims=True)\n",
    "    \n",
    "    # 5. Apply attention to values\n",
    "    output = np.matmul(attention_weights, V)  # (seq_len_q, d_v)\n",
    "    \n",
    "    return output, attention_weights\n",
    "\n",
    "print(\"Scaled dot-product attention implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Simple Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create simple example\n",
    "# Imagine 3 input tokens, each with 4-dimensional embeddings\n",
    "seq_len = 3\n",
    "d_model = 4\n",
    "\n",
    "# For simplicity, Q = K = V (self-attention)\n",
    "X = np.array([[1.0, 0.0, 1.0, 0.0],  # Token 1\n",
    "              [0.0, 2.0, 0.0, 2.0],  # Token 2\n",
    "              [1.0, 1.0, 1.0, 1.0]]) # Token 3\n",
    "\n",
    "Q = K = V = X\n",
    "\n",
    "# Compute attention\n",
    "output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "\n",
    "print(\"Input tokens (Q=K=V):\")\n",
    "print(X)\n",
    "print(f\"\\nAttention weights shape: {attention_weights.shape}\")\n",
    "print(\"\\nAttention weights (how much each output attends to each input):\")\n",
    "print(attention_weights)\n",
    "print(\"\\nOutput (weighted combination of values):\")\n",
    "print(output)\n",
    "\n",
    "# Visualize attention weights\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(attention_weights, annot=True, fmt='.3f', cmap='Blues',\n",
    "            xticklabels=['Token 1', 'Token 2', 'Token 3'],\n",
    "            yticklabels=['Token 1', 'Token 2', 'Token 3'])\n",
    "plt.xlabel('Attending to (Keys)')\n",
    "plt.ylabel('Attending from (Queries)')\n",
    "plt.title('Attention Weights Matrix')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Rows sum to 1 (each query attends to all keys with total weight = 1)\")\n",
    "print(\"- Higher values = stronger attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Self-Attention\n",
    "\n",
    "**Self-attention** is when a sequence attends to itself. This is the key mechanism in Transformers!\n",
    "\n",
    "### Process:\n",
    "1. Start with input sequence $X = [x_1, x_2, ..., x_n]$\n",
    "2. Create Q, K, V by linear projections:\n",
    "   - $Q = XW_Q$\n",
    "   - $K = XW_K$\n",
    "   - $V = XW_V$\n",
    "3. Apply attention: $\\text{Attention}(Q, K, V)$\n",
    "\n",
    "### Why is this powerful?\n",
    "Each token can look at **every other token** in the sequence to understand context!\n",
    "\n",
    "Example: \"The animal didn't cross the street because it was too tired\"\n",
    "- \"it\" should attend strongly to \"animal\", not \"street\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfAttention:\n",
    "    \"\"\"\n",
    "    Self-attention layer\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, d_k, d_v):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Input/output dimension\n",
    "            d_k: Dimension of queries and keys\n",
    "            d_v: Dimension of values\n",
    "        \"\"\"\n",
    "        self.d_k = d_k\n",
    "        \n",
    "        # Initialize projection matrices\n",
    "        self.W_q = np.random.randn(d_model, d_k) / np.sqrt(d_model)\n",
    "        self.W_k = np.random.randn(d_model, d_k) / np.sqrt(d_model)\n",
    "        self.W_v = np.random.randn(d_model, d_v) / np.sqrt(d_model)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            X: Input sequence (seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            output: Self-attention output (seq_len, d_v)\n",
    "            attention_weights: Attention distribution\n",
    "        \"\"\"\n",
    "        # Project to Q, K, V\n",
    "        Q = X @ self.W_q  # (seq_len, d_k)\n",
    "        K = X @ self.W_k  # (seq_len, d_k)\n",
    "        V = X @ self.W_v  # (seq_len, d_v)\n",
    "        \n",
    "        # Apply scaled dot-product attention\n",
    "        output, attention_weights = scaled_dot_product_attention(Q, K, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test\n",
    "np.random.seed(42)\n",
    "seq_len = 5\n",
    "d_model = 8\n",
    "d_k = d_v = 8\n",
    "\n",
    "# Random input sequence\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "\n",
    "# Create self-attention layer\n",
    "self_attn = SelfAttention(d_model, d_k, d_v)\n",
    "output, attn_weights = self_attn.forward(X)\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(attn_weights, annot=True, fmt='.2f', cmap='viridis',\n",
    "            xticklabels=range(1, seq_len+1),\n",
    "            yticklabels=range(1, seq_len+1))\n",
    "plt.xlabel('Attending to (position)')\n",
    "plt.ylabel('Attending from (position)')\n",
    "plt.title('Self-Attention Weights')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Multi-Head Attention\n",
    "\n",
    "Instead of performing a single attention, use **multiple attention heads** in parallel!\n",
    "\n",
    "### Why Multiple Heads?\n",
    "Different heads can learn to attend to different aspects:\n",
    "- Head 1: Syntactic relationships\n",
    "- Head 2: Semantic relationships\n",
    "- Head 3: Long-range dependencies\n",
    "- etc.\n",
    "\n",
    "### Formula:\n",
    "\n",
    "$\\text{head}_i = \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)$\n",
    "\n",
    "$\\text{MultiHead}(Q, K, V) = \\text{Concat}(\\text{head}_1, ..., \\text{head}_h)W^O$\n",
    "\n",
    "Where:\n",
    "- $h$ = number of heads\n",
    "- Each head has its own $W_i^Q, W_i^K, W_i^V$ matrices\n",
    "- $W^O$ projects concatenated heads back to $d_{model}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention:\n",
    "    \"\"\"\n",
    "    Multi-head attention mechanism\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            d_model: Model dimension\n",
    "            num_heads: Number of attention heads\n",
    "        \"\"\"\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads  # dimension per head\n",
    "        \n",
    "        # Projection matrices for all heads (combined)\n",
    "        self.W_q = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        self.W_k = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        self.W_v = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "        self.W_o = np.random.randn(d_model, d_model) / np.sqrt(d_model)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        Split into multiple heads\n",
    "        \n",
    "        Args:\n",
    "            x: (seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            (num_heads, seq_len, d_k)\n",
    "        \"\"\"\n",
    "        seq_len = x.shape[0]\n",
    "        # Reshape to (seq_len, num_heads, d_k)\n",
    "        x = x.reshape(seq_len, self.num_heads, self.d_k)\n",
    "        # Transpose to (num_heads, seq_len, d_k)\n",
    "        return x.transpose(1, 0, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        Combine multiple heads\n",
    "        \n",
    "        Args:\n",
    "            x: (num_heads, seq_len, d_k)\n",
    "        \n",
    "        Returns:\n",
    "            (seq_len, d_model)\n",
    "        \"\"\"\n",
    "        # Transpose to (seq_len, num_heads, d_k)\n",
    "        x = x.transpose(1, 0, 2)\n",
    "        seq_len = x.shape[0]\n",
    "        # Reshape to (seq_len, d_model)\n",
    "        return x.reshape(seq_len, self.d_model)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            X: Input (seq_len, d_model)\n",
    "        \n",
    "        Returns:\n",
    "            output: (seq_len, d_model)\n",
    "            attention_weights: list of attention matrices for each head\n",
    "        \"\"\"\n",
    "        # Project to Q, K, V\n",
    "        Q = X @ self.W_q\n",
    "        K = X @ self.W_k\n",
    "        V = X @ self.W_v\n",
    "        \n",
    "        # Split into multiple heads\n",
    "        Q_heads = self.split_heads(Q)  # (num_heads, seq_len, d_k)\n",
    "        K_heads = self.split_heads(K)\n",
    "        V_heads = self.split_heads(V)\n",
    "        \n",
    "        # Apply attention for each head\n",
    "        outputs = []\n",
    "        attention_weights = []\n",
    "        \n",
    "        for i in range(self.num_heads):\n",
    "            output_i, attn_i = scaled_dot_product_attention(\n",
    "                Q_heads[i], K_heads[i], V_heads[i]\n",
    "            )\n",
    "            outputs.append(output_i)\n",
    "            attention_weights.append(attn_i)\n",
    "        \n",
    "        # Stack and combine heads\n",
    "        outputs = np.stack(outputs)  # (num_heads, seq_len, d_k)\n",
    "        output = self.combine_heads(outputs)  # (seq_len, d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = output @ self.W_o\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test multi-head attention\n",
    "np.random.seed(42)\n",
    "seq_len = 6\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "\n",
    "mha = MultiHeadAttention(d_model, num_heads)\n",
    "output, attn_weights = mha.forward(X)\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Number of attention heads: {len(attn_weights)}\")\n",
    "print(f\"Each head attention shape: {attn_weights[0].shape}\")\n",
    "\n",
    "# Visualize each head\n",
    "fig, axes = plt.subplots(1, num_heads, figsize=(12, 5))\n",
    "\n",
    "for i in range(num_heads):\n",
    "    ax = axes[i] if num_heads > 1 else axes\n",
    "    sns.heatmap(attn_weights[i], annot=True, fmt='.2f', cmap='viridis', ax=ax)\n",
    "    ax.set_title(f'Head {i+1} Attention')\n",
    "    ax.set_xlabel('Key position')\n",
    "    ax.set_ylabel('Query position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice: Different heads learn different attention patterns!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PyTorch Implementation\n",
    "\n",
    "Now let's see how clean this is in PyTorch!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttentionPyTorch(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-head attention in PyTorch\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def split_heads(self, x):\n",
    "        \"\"\"Split into multiple heads\"\"\"\n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        # (batch, seq_len, d_model) -> (batch, seq_len, num_heads, d_k)\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.d_k)\n",
    "        # (batch, seq_len, num_heads, d_k) -> (batch, num_heads, seq_len, d_k)\n",
    "        return x.transpose(1, 2)\n",
    "    \n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"Combine multiple heads\"\"\"\n",
    "        batch_size, num_heads, seq_len, d_k = x.size()\n",
    "        # (batch, num_heads, seq_len, d_k) -> (batch, seq_len, num_heads, d_k)\n",
    "        x = x.transpose(1, 2).contiguous()\n",
    "        # (batch, seq_len, num_heads, d_k) -> (batch, seq_len, d_model)\n",
    "        return x.view(batch_size, seq_len, self.d_model)\n",
    "    \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"Compute scaled dot-product attention\"\"\"\n",
    "        # Q, K, V: (batch, num_heads, seq_len, d_k)\n",
    "        d_k = Q.size(-1)\n",
    "        \n",
    "        # Attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / np.sqrt(d_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        # Attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply to values\n",
    "        output = torch.matmul(attention_weights, V)\n",
    "        \n",
    "        return output, attention_weights\n",
    "    \n",
    "    def forward(self, X, mask=None):\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \n",
    "        Args:\n",
    "            X: (batch, seq_len, d_model)\n",
    "            mask: Optional mask\n",
    "        \n",
    "        Returns:\n",
    "            output: (batch, seq_len, d_model)\n",
    "            attention_weights: (batch, num_heads, seq_len, seq_len)\n",
    "        \"\"\"\n",
    "        batch_size = X.size(0)\n",
    "        \n",
    "        # Linear projections\n",
    "        Q = self.W_q(X)  # (batch, seq_len, d_model)\n",
    "        K = self.W_k(X)\n",
    "        V = self.W_v(X)\n",
    "        \n",
    "        # Split into heads\n",
    "        Q = self.split_heads(Q)  # (batch, num_heads, seq_len, d_k)\n",
    "        K = self.split_heads(K)\n",
    "        V = self.split_heads(V)\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_output, attention_weights = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        \n",
    "        # Combine heads\n",
    "        attn_output = self.combine_heads(attn_output)  # (batch, seq_len, d_model)\n",
    "        \n",
    "        # Final projection\n",
    "        output = self.W_o(attn_output)\n",
    "        \n",
    "        return output, attention_weights\n",
    "\n",
    "# Test\n",
    "batch_size = 2\n",
    "seq_len = 6\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "\n",
    "X = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "mha_torch = MultiHeadAttentionPyTorch(d_model, num_heads)\n",
    "output, attn_weights = mha_torch(X)\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "\n",
    "# Visualize attention for first sample\n",
    "fig, axes = plt.subplots(1, num_heads, figsize=(12, 5))\n",
    "\n",
    "for i in range(num_heads):\n",
    "    ax = axes[i] if num_heads > 1 else axes\n",
    "    attn_map = attn_weights[0, i].detach().numpy()  # first sample, i-th head\n",
    "    sns.heatmap(attn_map, annot=True, fmt='.2f', cmap='viridis', ax=ax)\n",
    "    ax.set_title(f'Head {i+1} Attention (PyTorch)')\n",
    "    ax.set_xlabel('Key position')\n",
    "    ax.set_ylabel('Query position')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Using PyTorch's Built-in MultiheadAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch provides nn.MultiheadAttention out of the box!\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "seq_len = 6\n",
    "batch_size = 2\n",
    "\n",
    "# Create module\n",
    "multihead_attn = nn.MultiheadAttention(embed_dim=d_model, num_heads=num_heads, batch_first=True)\n",
    "\n",
    "# Input\n",
    "X = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Forward (Q, K, V can be the same for self-attention)\n",
    "attn_output, attn_weights = multihead_attn(X, X, X, need_weights=True, average_attn_weights=False)\n",
    "\n",
    "print(f\"Input shape: {X.shape}\")\n",
    "print(f\"Output shape: {attn_output.shape}\")\n",
    "print(f\"Attention weights shape: {attn_weights.shape}\")\n",
    "print(\"\\nUsing PyTorch's built-in is much easier!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Masked Attention (for Autoregressive Models)\n",
    "\n",
    "In language models like GPT, we need **causal masking** so that position $i$ can only attend to positions $\\leq i$.\n",
    "\n",
    "This prevents the model from \"cheating\" by looking at future tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Create a causal mask for autoregressive attention\n",
    "    \n",
    "    Returns:\n",
    "        mask: (seq_len, seq_len) lower triangular matrix\n",
    "    \"\"\"\n",
    "    mask = np.tril(np.ones((seq_len, seq_len)))\n",
    "    return mask\n",
    "\n",
    "# Example\n",
    "seq_len = 5\n",
    "mask = create_causal_mask(seq_len)\n",
    "\n",
    "print(\"Causal Mask (1 = allowed, 0 = masked):\")\n",
    "print(mask.astype(int))\n",
    "\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(mask, annot=True, fmt='.0f', cmap='Blues', cbar=False,\n",
    "            xticklabels=range(1, seq_len+1),\n",
    "            yticklabels=range(1, seq_len+1))\n",
    "plt.xlabel('Can attend to position')\n",
    "plt.ylabel('From position')\n",
    "plt.title('Causal Attention Mask')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- Position 1 can only attend to itself\")\n",
    "print(\"- Position 2 can attend to positions 1 and 2\")\n",
    "print(\"- Position 3 can attend to positions 1, 2, and 3\")\n",
    "print(\"- etc.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply causal masking to attention\n",
    "seq_len = 5\n",
    "d_model = 4\n",
    "\n",
    "X = np.random.randn(seq_len, d_model)\n",
    "Q = K = V = X\n",
    "\n",
    "# Create causal mask\n",
    "causal_mask = 1 - create_causal_mask(seq_len)  # invert: 1 = mask, 0 = allow\n",
    "\n",
    "# Compute attention with mask\n",
    "output, attn_weights = scaled_dot_product_attention(Q, K, V, mask=causal_mask)\n",
    "\n",
    "print(\"Attention weights with causal masking:\")\n",
    "print(attn_weights)\n",
    "\n",
    "plt.figure(figsize=(7, 6))\n",
    "sns.heatmap(attn_weights, annot=True, fmt='.3f', cmap='viridis',\n",
    "            xticklabels=range(1, seq_len+1),\n",
    "            yticklabels=range(1, seq_len+1))\n",
    "plt.xlabel('Attending to (position)')\n",
    "plt.ylabel('Attending from (position)')\n",
    "plt.title('Causal Self-Attention Weights')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNotice: Upper triangle is all zeros (can't attend to future!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Positional Encoding\n",
    "\n",
    "**Problem:** Attention has no notion of position! The attention mechanism is **permutation invariant**.\n",
    "\n",
    "**Solution:** Add positional information to the input embeddings.\n",
    "\n",
    "### Sinusoidal Positional Encoding:\n",
    "\n",
    "$PE_{(pos, 2i)} = \\sin\\left(\\frac{pos}{10000^{2i/d}}\\right)$\n",
    "\n",
    "$PE_{(pos, 2i+1)} = \\cos\\left(\\frac{pos}{10000^{2i/d}}\\right)$\n",
    "\n",
    "where:\n",
    "- $pos$ = position in sequence\n",
    "- $i$ = dimension index\n",
    "- $d$ = model dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(seq_len, d_model):\n",
    "    \"\"\"\n",
    "    Generate sinusoidal positional encodings\n",
    "    \n",
    "    Args:\n",
    "        seq_len: Sequence length\n",
    "        d_model: Model dimension\n",
    "    \n",
    "    Returns:\n",
    "        PE: (seq_len, d_model) positional encoding matrix\n",
    "    \"\"\"\n",
    "    PE = np.zeros((seq_len, d_model))\n",
    "    \n",
    "    for pos in range(seq_len):\n",
    "        for i in range(0, d_model, 2):\n",
    "            # Even indices: sin\n",
    "            PE[pos, i] = np.sin(pos / (10000 ** (2*i / d_model)))\n",
    "            # Odd indices: cos\n",
    "            if i + 1 < d_model:\n",
    "                PE[pos, i+1] = np.cos(pos / (10000 ** (2*i / d_model)))\n",
    "    \n",
    "    return PE\n",
    "\n",
    "# Generate positional encodings\n",
    "seq_len = 50\n",
    "d_model = 128\n",
    "\n",
    "PE = positional_encoding(seq_len, d_model)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(PE, cmap='RdBu', center=0, cbar=True)\n",
    "plt.xlabel('Dimension')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Positional Encoding Matrix')\n",
    "plt.show()\n",
    "\n",
    "# Plot some dimensions\n",
    "plt.figure(figsize=(12, 5))\n",
    "for i in range(0, d_model, 16):\n",
    "    plt.plot(PE[:, i], label=f'Dim {i}')\n",
    "plt.xlabel('Position')\n",
    "plt.ylabel('Encoding Value')\n",
    "plt.title('Positional Encoding - Selected Dimensions')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"Key properties:\")\n",
    "print(\"- Each position has a unique encoding\")\n",
    "print(\"- Periodic patterns at different frequencies\")\n",
    "print(\"- Model can learn to attend by relative position\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Attention Mechanism**\n",
    "   - Allows dynamic focus on relevant parts of input\n",
    "   - Query, Key, Value paradigm\n",
    "   - Solves fixed-context problem of RNNs\n",
    "\n",
    "2. **Scaled Dot-Product Attention**\n",
    "   - Formula: $\\text{Attention}(Q,K,V) = \\text{softmax}(\\frac{QK^T}{\\sqrt{d_k}})V$\n",
    "   - Computes weighted sum of values\n",
    "   - Weights based on query-key similarity\n",
    "\n",
    "3. **Self-Attention**\n",
    "   - Sequence attends to itself\n",
    "   - Foundation of Transformers\n",
    "   - Each token can look at all other tokens\n",
    "\n",
    "4. **Multi-Head Attention**\n",
    "   - Multiple attention mechanisms in parallel\n",
    "   - Learn different types of relationships\n",
    "   - Concat + project to combine\n",
    "\n",
    "5. **Causal Masking**\n",
    "   - Prevent attending to future positions\n",
    "   - Essential for autoregressive models (GPT)\n",
    "\n",
    "6. **Positional Encoding**\n",
    "   - Add position information\n",
    "   - Sinusoidal or learned embeddings\n",
    "\n",
    "### Why Attention is Revolutionary:\n",
    "\n",
    "- **Parallelization**: Unlike RNNs, can process entire sequence at once\n",
    "- **Long-range dependencies**: Direct connections between any positions\n",
    "- **Interpretability**: Attention weights show what model focuses on\n",
    "- **Flexibility**: Works for various tasks (translation, generation, etc.)\n",
    "\n",
    "### Applications:\n",
    "\n",
    "- **NLP**: GPT, BERT, T5 (all use attention)\n",
    "- **Vision**: Vision Transformers (ViT)\n",
    "- **Multimodal**: CLIP, Flamingo\n",
    "- **Generation**: Stable Diffusion, Midjourney\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "To build a complete Transformer:\n",
    "1. Multi-head attention (âœ“ we have this!)\n",
    "2. Feed-forward networks\n",
    "3. Layer normalization\n",
    "4. Residual connections\n",
    "5. Stack multiple layers\n",
    "\n",
    "You now understand the core mechanism that powers modern AI!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Implement cross-attention\n",
    "# Cross-attention: Q from one sequence, K and V from another\n",
    "# Used in encoder-decoder architectures\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Visualize attention patterns on real text\n",
    "# Use a pre-trained model to see what it attends to\n",
    "# Example sentence: \"The cat sat on the mat because it was comfortable\"\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Implement learned positional embeddings\n",
    "# Instead of sinusoidal, use nn.Embedding for positions\n",
    "# Your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
