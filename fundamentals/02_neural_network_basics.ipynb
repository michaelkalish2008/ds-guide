{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Basics\n",
    "\n",
    "In this notebook, we'll build our understanding of neural networks from the ground up:\n",
    "1. **The Perceptron** - The simplest neural unit\n",
    "2. **Activation Functions** - Adding non-linearity\n",
    "3. **Forward Propagation** - Computing predictions\n",
    "4. **Loss Functions** - Measuring errors\n",
    "\n",
    "We'll implement everything in NumPy first to understand the mechanics, then see how PyTorch simplifies it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The Perceptron: The Building Block\n",
    "\n",
    "A **perceptron** (or neuron) is the fundamental unit of a neural network.\n",
    "\n",
    "### How it works:\n",
    "1. Takes multiple inputs: $x_1, x_2, ..., x_n$\n",
    "2. Multiplies each by a weight: $w_1, w_2, ..., w_n$\n",
    "3. Sums them with a bias: $z = w_1x_1 + w_2x_2 + ... + w_nx_n + b$\n",
    "4. Applies an activation function: $a = \\sigma(z)$\n",
    "\n",
    "### Vector notation:\n",
    "$z = w^T x + b$ (or $z = w \\cdot x + b$)\n",
    "\n",
    "$a = \\sigma(z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    \"\"\"A single perceptron/neuron\"\"\"\n",
    "    \n",
    "    def __init__(self, n_inputs):\n",
    "        \"\"\"Initialize with random weights and bias\"\"\"\n",
    "        self.weights = np.random.randn(n_inputs) * 0.1\n",
    "        self.bias = np.random.randn() * 0.1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Compute output given input x\"\"\"\n",
    "        # Linear combination: z = w·x + b\n",
    "        z = np.dot(self.weights, x) + self.bias\n",
    "        return z\n",
    "    \n",
    "# Example: perceptron with 3 inputs\n",
    "perceptron = Perceptron(n_inputs=3)\n",
    "x = np.array([1.0, 2.0, 3.0])\n",
    "\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Weights: {perceptron.weights}\")\n",
    "print(f\"Bias: {perceptron.bias}\")\n",
    "print(f\"Output (z): {perceptron.forward(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Activation Functions\n",
    "\n",
    "**Why do we need activation functions?**\n",
    "\n",
    "Without activation functions, stacking multiple layers would just be a linear transformation:\n",
    "- $y = W_2(W_1x + b_1) + b_2 = (W_2W_1)x + (W_2b_1 + b_2) = W'x + b'$\n",
    "\n",
    "This is equivalent to a single layer! Activation functions add **non-linearity**, enabling networks to learn complex patterns.\n",
    "\n",
    "### Common Activation Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Sigmoid\n",
    "\n",
    "**Formula:** $\\sigma(z) = \\frac{1}{1 + e^{-z}}$\n",
    "\n",
    "**Properties:**\n",
    "- Output range: (0, 1)\n",
    "- Smooth, differentiable\n",
    "- Used for binary classification\n",
    "- **Problem:** Vanishing gradients for large |z|\n",
    "\n",
    "**Derivative:** $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"Sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    \"\"\"Derivative of sigmoid\"\"\"\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Visualize\n",
    "z = np.linspace(-10, 10, 200)\n",
    "y = sigmoid(z)\n",
    "dy = sigmoid_derivative(z)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(z, y, linewidth=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('σ(z)')\n",
    "plt.title('Sigmoid Function')\n",
    "plt.axhline(y=0.5, color='r', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(z, dy, linewidth=2, color='orange')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel(\"σ'(z)\")\n",
    "plt.title('Sigmoid Derivative (notice vanishing at extremes)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Tanh (Hyperbolic Tangent)\n",
    "\n",
    "**Formula:** $\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$\n",
    "\n",
    "**Properties:**\n",
    "- Output range: (-1, 1)\n",
    "- Zero-centered (better than sigmoid)\n",
    "- Still has vanishing gradient problem\n",
    "\n",
    "**Derivative:** $\\tanh'(z) = 1 - \\tanh^2(z)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tanh(z):\n",
    "    \"\"\"Tanh activation function\"\"\"\n",
    "    return np.tanh(z)\n",
    "\n",
    "def tanh_derivative(z):\n",
    "    \"\"\"Derivative of tanh\"\"\"\n",
    "    return 1 - np.tanh(z)**2\n",
    "\n",
    "# Visualize\n",
    "y_tanh = tanh(z)\n",
    "dy_tanh = tanh_derivative(z)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(z, y_tanh, linewidth=2, color='green')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('tanh(z)')\n",
    "plt.title('Tanh Function')\n",
    "plt.axhline(y=0, color='r', linestyle='--', alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(z, dy_tanh, linewidth=2, color='orange')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel(\"tanh'(z)\")\n",
    "plt.title('Tanh Derivative')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 ReLU (Rectified Linear Unit) - Most Popular!\n",
    "\n",
    "**Formula:** $\\text{ReLU}(z) = \\max(0, z)$\n",
    "\n",
    "**Properties:**\n",
    "- Output range: [0, ∞)\n",
    "- Computationally efficient\n",
    "- No vanishing gradient for positive values\n",
    "- **Default choice** for hidden layers\n",
    "- **Problem:** \"Dead ReLU\" (neurons that always output 0)\n",
    "\n",
    "**Derivative:**\n",
    "$\\text{ReLU}'(z) = \\begin{cases} 1 & \\text{if } z > 0 \\\\ 0 & \\text{if } z \\leq 0 \\end{cases}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z):\n",
    "    \"\"\"ReLU activation function\"\"\"\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    \"\"\"Derivative of ReLU\"\"\"\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "# Visualize\n",
    "y_relu = relu(z)\n",
    "dy_relu = relu_derivative(z)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(z, y_relu, linewidth=2, color='red')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('ReLU(z)')\n",
    "plt.title('ReLU Function')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(z, dy_relu, linewidth=2, color='orange')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel(\"ReLU'(z)\")\n",
    "plt.title('ReLU Derivative')\n",
    "plt.ylim(-0.1, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Comparing All Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare all three\n",
    "plt.figure(figsize=(14, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(z, sigmoid(z), label='Sigmoid', linewidth=2)\n",
    "plt.plot(z, tanh(z), label='Tanh', linewidth=2)\n",
    "plt.plot(z, relu(z), label='ReLU', linewidth=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('Activation')\n",
    "plt.title('Activation Functions')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(z, sigmoid_derivative(z), label='Sigmoid', linewidth=2)\n",
    "plt.plot(z, tanh_derivative(z), label='Tanh', linewidth=2)\n",
    "plt.plot(z, relu_derivative(z), label='ReLU', linewidth=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('z')\n",
    "plt.ylabel('Derivative')\n",
    "plt.title('Derivatives (gradients)')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Key Observation: ReLU derivative is always 0 or 1 (no vanishing gradient!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Forward Propagation\n",
    "\n",
    "Forward propagation is the process of computing the network's output given an input.\n",
    "\n",
    "### For a single layer:\n",
    "1. **Linear transformation:** $Z = WX + b$\n",
    "2. **Activation:** $A = \\sigma(Z)$\n",
    "\n",
    "### For multiple layers:\n",
    "1. $Z^{[1]} = W^{[1]}X + b^{[1]}$\n",
    "2. $A^{[1]} = \\sigma(Z^{[1]})$\n",
    "3. $Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$\n",
    "4. $A^{[2]} = \\sigma(Z^{[2]})$\n",
    "5. And so on..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Single Neuron Forward Pass - NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Single neuron with ReLU activation\n",
    "def forward_neuron(x, w, b, activation='relu'):\n",
    "    \"\"\"\n",
    "    Forward pass for a single neuron\n",
    "    \n",
    "    Args:\n",
    "        x: input vector\n",
    "        w: weight vector\n",
    "        b: bias (scalar)\n",
    "        activation: activation function name\n",
    "    \n",
    "    Returns:\n",
    "        z: pre-activation\n",
    "        a: post-activation\n",
    "    \"\"\"\n",
    "    # Linear transformation\n",
    "    z = np.dot(w, x) + b\n",
    "    \n",
    "    # Apply activation\n",
    "    if activation == 'relu':\n",
    "        a = relu(z)\n",
    "    elif activation == 'sigmoid':\n",
    "        a = sigmoid(z)\n",
    "    elif activation == 'tanh':\n",
    "        a = tanh(z)\n",
    "    else:\n",
    "        a = z  # linear\n",
    "    \n",
    "    return z, a\n",
    "\n",
    "# Test it\n",
    "x = np.array([1.0, 2.0, 3.0])\n",
    "w = np.array([0.5, -0.2, 0.1])\n",
    "b = 0.3\n",
    "\n",
    "z, a = forward_neuron(x, w, b, activation='relu')\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Weights: {w}\")\n",
    "print(f\"Bias: {b}\")\n",
    "print(f\"Pre-activation (z): {z:.4f}\")\n",
    "print(f\"Post-activation (a): {a:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Layer Forward Pass - NumPy\n",
    "\n",
    "A layer has multiple neurons. We can compute all outputs in parallel using matrix multiplication!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_layer(X, W, b, activation='relu'):\n",
    "    \"\"\"\n",
    "    Forward pass for a layer\n",
    "    \n",
    "    Args:\n",
    "        X: input matrix (features x samples)\n",
    "        W: weight matrix (neurons x features)\n",
    "        b: bias vector (neurons,)\n",
    "        activation: activation function\n",
    "    \n",
    "    Returns:\n",
    "        Z: pre-activation\n",
    "        A: post-activation\n",
    "    \"\"\"\n",
    "    # Linear transformation: Z = WX + b\n",
    "    Z = W @ X + b.reshape(-1, 1)  # broadcast bias\n",
    "    \n",
    "    # Apply activation\n",
    "    if activation == 'relu':\n",
    "        A = relu(Z)\n",
    "    elif activation == 'sigmoid':\n",
    "        A = sigmoid(Z)\n",
    "    elif activation == 'tanh':\n",
    "        A = tanh(Z)\n",
    "    else:\n",
    "        A = Z\n",
    "    \n",
    "    return Z, A\n",
    "\n",
    "# Example: 3 inputs, 4 neurons, 2 samples\n",
    "X = np.array([[1.0, 2.0],    # sample 1 and 2 for feature 1\n",
    "              [2.0, 3.0],    # sample 1 and 2 for feature 2\n",
    "              [3.0, 4.0]])   # sample 1 and 2 for feature 3\n",
    "\n",
    "W = np.random.randn(4, 3) * 0.1  # 4 neurons, 3 inputs each\n",
    "b = np.random.randn(4) * 0.1     # 4 biases\n",
    "\n",
    "Z, A = forward_layer(X, W, b, activation='relu')\n",
    "\n",
    "print(f\"Input shape: {X.shape} (3 features, 2 samples)\")\n",
    "print(f\"Weight shape: {W.shape} (4 neurons, 3 inputs)\")\n",
    "print(f\"Bias shape: {b.shape}\")\n",
    "print(f\"\\nOutput shape: {A.shape} (4 neurons, 2 samples)\")\n",
    "print(f\"\\nOutput values:\\n{A}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Multi-Layer Forward Pass - NumPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_network(X, parameters):\n",
    "    \"\"\"\n",
    "    Forward propagation through multiple layers\n",
    "    \n",
    "    Args:\n",
    "        X: input (features x samples)\n",
    "        parameters: dict with W1, b1, W2, b2, ...\n",
    "    \n",
    "    Returns:\n",
    "        cache: dict storing all intermediate values\n",
    "    \"\"\"\n",
    "    cache = {'A0': X}\n",
    "    A = X\n",
    "    L = len(parameters) // 2  # number of layers\n",
    "    \n",
    "    # Forward through hidden layers with ReLU\n",
    "    for l in range(1, L):\n",
    "        W = parameters[f'W{l}']\n",
    "        b = parameters[f'b{l}']\n",
    "        Z = W @ A + b.reshape(-1, 1)\n",
    "        A = relu(Z)\n",
    "        cache[f'Z{l}'] = Z\n",
    "        cache[f'A{l}'] = A\n",
    "    \n",
    "    # Output layer (no activation for now)\n",
    "    W = parameters[f'W{L}']\n",
    "    b = parameters[f'b{L}']\n",
    "    Z = W @ A + b.reshape(-1, 1)\n",
    "    A = Z  # linear output\n",
    "    cache[f'Z{L}'] = Z\n",
    "    cache[f'A{L}'] = A\n",
    "    \n",
    "    return cache\n",
    "\n",
    "# Example: 3 -> 4 -> 2 network\n",
    "parameters = {\n",
    "    'W1': np.random.randn(4, 3) * 0.1,\n",
    "    'b1': np.random.randn(4) * 0.1,\n",
    "    'W2': np.random.randn(2, 4) * 0.1,\n",
    "    'b2': np.random.randn(2) * 0.1,\n",
    "}\n",
    "\n",
    "X = np.array([[1.0, 2.0],\n",
    "              [2.0, 3.0],\n",
    "              [3.0, 4.0]])\n",
    "\n",
    "cache = forward_network(X, parameters)\n",
    "\n",
    "print(\"Network architecture: 3 inputs -> 4 hidden -> 2 outputs\")\n",
    "print(f\"\\nInput (A0): shape {cache['A0'].shape}\")\n",
    "print(cache['A0'])\n",
    "print(f\"\\nHidden layer (A1): shape {cache['A1'].shape}\")\n",
    "print(cache['A1'])\n",
    "print(f\"\\nOutput (A2): shape {cache['A2'].shape}\")\n",
    "print(cache['A2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Loss Functions\n",
    "\n",
    "Loss functions measure how wrong our predictions are. During training, we minimize the loss.\n",
    "\n",
    "### Common Loss Functions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Mean Squared Error (MSE) - Regression\n",
    "\n",
    "**Formula:** $L = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2$\n",
    "\n",
    "Used for regression tasks (predicting continuous values)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"Mean Squared Error\"\"\"\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "# Example\n",
    "y_true = np.array([1.0, 2.0, 3.0])\n",
    "y_pred = np.array([1.1, 2.3, 2.8])\n",
    "\n",
    "loss = mse_loss(y_true, y_pred)\n",
    "print(f\"True values: {y_true}\")\n",
    "print(f\"Predictions: {y_pred}\")\n",
    "print(f\"MSE Loss: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Binary Cross-Entropy - Binary Classification\n",
    "\n",
    "**Formula:** $L = -\\frac{1}{n} \\sum_{i=1}^{n} [y_i \\log(\\hat{y}_i) + (1-y_i) \\log(1-\\hat{y}_i)]$\n",
    "\n",
    "Used when classifying into 2 classes (0 or 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"Binary Cross-Entropy Loss\"\"\"\n",
    "    # Clip predictions to avoid log(0)\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "# Example\n",
    "y_true = np.array([1, 0, 1, 1, 0])\n",
    "y_pred = np.array([0.9, 0.1, 0.8, 0.7, 0.2])\n",
    "\n",
    "loss = binary_cross_entropy(y_true, y_pred)\n",
    "print(f\"True labels: {y_true}\")\n",
    "print(f\"Predicted probabilities: {y_pred}\")\n",
    "print(f\"Binary Cross-Entropy: {loss:.4f}\")\n",
    "\n",
    "# Compare with bad predictions\n",
    "y_pred_bad = np.array([0.3, 0.6, 0.4, 0.2, 0.8])\n",
    "loss_bad = binary_cross_entropy(y_true, y_pred_bad)\n",
    "print(f\"\\nBad predictions: {y_pred_bad}\")\n",
    "print(f\"Binary Cross-Entropy: {loss_bad:.4f} (higher is worse!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Categorical Cross-Entropy - Multi-class Classification\n",
    "\n",
    "**Formula:** $L = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} y_{i,c} \\log(\\hat{y}_{i,c})$\n",
    "\n",
    "Used when classifying into multiple classes (e.g., cat, dog, bird)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorical_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Categorical Cross-Entropy Loss\n",
    "    \n",
    "    Args:\n",
    "        y_true: one-hot encoded true labels (samples x classes)\n",
    "        y_pred: predicted probabilities (samples x classes)\n",
    "    \"\"\"\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=1))\n",
    "\n",
    "# Example: 3 classes, 4 samples\n",
    "y_true = np.array([[1, 0, 0],  # class 0\n",
    "                   [0, 1, 0],  # class 1\n",
    "                   [0, 0, 1],  # class 2\n",
    "                   [1, 0, 0]]) # class 0\n",
    "\n",
    "y_pred = np.array([[0.7, 0.2, 0.1],\n",
    "                   [0.1, 0.8, 0.1],\n",
    "                   [0.2, 0.2, 0.6],\n",
    "                   [0.8, 0.1, 0.1]])\n",
    "\n",
    "loss = categorical_cross_entropy(y_true, y_pred)\n",
    "print(f\"True labels (one-hot):\\n{y_true}\")\n",
    "print(f\"\\nPredicted probabilities:\\n{y_pred}\")\n",
    "print(f\"\\nCategorical Cross-Entropy: {loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Putting It All Together: A Complete Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate simple dataset: XOR problem\n",
    "# XOR is not linearly separable, needs a hidden layer!\n",
    "X = np.array([[0, 0],\n",
    "              [0, 1],\n",
    "              [1, 0],\n",
    "              [1, 1]]).T  # transpose to make it (features x samples)\n",
    "\n",
    "y = np.array([[0],\n",
    "              [1],\n",
    "              [1],\n",
    "              [0]]).T\n",
    "\n",
    "print(\"XOR Dataset:\")\n",
    "print(f\"Inputs:\\n{X.T}\")\n",
    "print(f\"Targets:\\n{y.T}\")\n",
    "\n",
    "# Initialize small network: 2 -> 4 -> 1\n",
    "np.random.seed(42)\n",
    "params = {\n",
    "    'W1': np.random.randn(4, 2) * 0.5,\n",
    "    'b1': np.zeros(4),\n",
    "    'W2': np.random.randn(1, 4) * 0.5,\n",
    "    'b2': np.zeros(1),\n",
    "}\n",
    "\n",
    "# Forward pass\n",
    "Z1 = params['W1'] @ X + params['b1'].reshape(-1, 1)\n",
    "A1 = relu(Z1)\n",
    "Z2 = params['W2'] @ A1 + params['b2'].reshape(-1, 1)\n",
    "A2 = sigmoid(Z2)  # sigmoid for binary output\n",
    "\n",
    "print(f\"\\nHidden layer activations (A1):\\n{A1}\")\n",
    "print(f\"\\nOutput predictions (A2):\\n{A2}\")\n",
    "print(f\"\\nLoss (before training): {binary_cross_entropy(y, A2):.4f}\")\n",
    "print(\"\\nNote: Random weights give poor predictions. We'll learn to train this in the next notebook!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PyTorch Introduction\n",
    "\n",
    "PyTorch makes all of this much easier! Let's see how:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 PyTorch Tensors (like NumPy arrays, but GPU-enabled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NumPy array\n",
    "x_np = np.array([1.0, 2.0, 3.0])\n",
    "print(f\"NumPy array: {x_np}\")\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "x_torch = torch.tensor([1.0, 2.0, 3.0])\n",
    "print(f\"PyTorch tensor: {x_torch}\")\n",
    "\n",
    "# Or convert from NumPy\n",
    "x_torch2 = torch.from_numpy(x_np)\n",
    "print(f\"From NumPy: {x_torch2}\")\n",
    "\n",
    "# Convert back to NumPy\n",
    "x_back = x_torch.numpy()\n",
    "print(f\"Back to NumPy: {x_back}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Built-in Activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_torch = torch.tensor([-2.0, -1.0, 0.0, 1.0, 2.0])\n",
    "\n",
    "# ReLU\n",
    "relu_torch = torch.relu(z_torch)\n",
    "print(f\"Input: {z_torch}\")\n",
    "print(f\"ReLU: {relu_torch}\")\n",
    "\n",
    "# Sigmoid\n",
    "sigmoid_torch = torch.sigmoid(z_torch)\n",
    "print(f\"Sigmoid: {sigmoid_torch}\")\n",
    "\n",
    "# Tanh\n",
    "tanh_torch = torch.tanh(z_torch)\n",
    "print(f\"Tanh: {tanh_torch}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Building a Neural Network Layer in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a linear layer: 3 inputs -> 4 outputs\n",
    "layer = nn.Linear(in_features=3, out_features=4)\n",
    "\n",
    "print(f\"Weight shape: {layer.weight.shape}\")\n",
    "print(f\"Bias shape: {layer.bias.shape}\")\n",
    "\n",
    "# Forward pass\n",
    "x = torch.tensor([[1.0, 2.0, 3.0],\n",
    "                  [4.0, 5.0, 6.0]])\n",
    "\n",
    "output = layer(x)\n",
    "print(f\"\\nInput shape: {x.shape} (2 samples, 3 features)\")\n",
    "print(f\"Output shape: {output.shape} (2 samples, 4 neurons)\")\n",
    "print(f\"Output:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Building a Complete Network in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(2, 4)  # 2 inputs -> 4 hidden\n",
    "        self.layer2 = nn.Linear(4, 1)  # 4 hidden -> 1 output\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.layer1(x))  # hidden layer with ReLU\n",
    "        x = torch.sigmoid(self.layer2(x))  # output with sigmoid\n",
    "        return x\n",
    "\n",
    "# Create network\n",
    "model = SimpleNetwork()\n",
    "print(model)\n",
    "\n",
    "# Test with XOR data\n",
    "X_torch = torch.tensor([[0., 0.],\n",
    "                        [0., 1.],\n",
    "                        [1., 0.],\n",
    "                        [1., 1.]])\n",
    "\n",
    "y_torch = torch.tensor([[0.],\n",
    "                        [1.],\n",
    "                        [1.],\n",
    "                        [0.]])\n",
    "\n",
    "# Forward pass\n",
    "predictions = model(X_torch)\n",
    "print(f\"\\nPredictions (untrained):\\n{predictions}\")\n",
    "\n",
    "# Compute loss\n",
    "loss_fn = nn.BCELoss()  # Binary Cross-Entropy\n",
    "loss = loss_fn(predictions, y_torch)\n",
    "print(f\"\\nLoss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Perceptron/Neuron**: The basic unit\n",
    "   - Linear combination: $z = w^Tx + b$\n",
    "   - Activation function: $a = \\sigma(z)$\n",
    "\n",
    "2. **Activation Functions**:\n",
    "   - **Sigmoid**: Smooth, 0-1 range, has vanishing gradient\n",
    "   - **Tanh**: Smooth, -1 to 1, zero-centered\n",
    "   - **ReLU**: Most popular, simple, no vanishing gradient\n",
    "\n",
    "3. **Forward Propagation**:\n",
    "   - Pass data through layers sequentially\n",
    "   - Each layer: linear transform + activation\n",
    "   - Cache intermediate values for backprop\n",
    "\n",
    "4. **Loss Functions**:\n",
    "   - **MSE**: For regression\n",
    "   - **Binary Cross-Entropy**: For binary classification\n",
    "   - **Categorical Cross-Entropy**: For multi-class\n",
    "\n",
    "5. **PyTorch**:\n",
    "   - Tensors (GPU-enabled arrays)\n",
    "   - Built-in layers (nn.Linear)\n",
    "   - Built-in activations and losses\n",
    "   - Much cleaner code!\n",
    "\n",
    "### Next Steps:\n",
    "In the next notebook, we'll build complete feedforward networks in both NumPy and PyTorch!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Implement LeakyReLU\n",
    "# LeakyReLU(z) = max(0.01*z, z)\n",
    "# This fixes the \"dead ReLU\" problem by allowing small negative values\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    # Your code here\n",
    "    pass\n",
    "\n",
    "# Test it\n",
    "# z_test = np.array([-2, -1, 0, 1, 2])\n",
    "# print(leaky_relu(z_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Compute forward pass for a 3->5->2 network manually\n",
    "# Use your own random weights and ReLU activation\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Build a 4-layer network in PyTorch (2->8->4->1)\n",
    "# Use ReLU for hidden layers and sigmoid for output\n",
    "# Your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
