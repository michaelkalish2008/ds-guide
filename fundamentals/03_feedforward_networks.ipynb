{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward Neural Networks\n",
    "\n",
    "In this notebook, we'll build complete feedforward neural networks (also called multilayer perceptrons or MLPs).\n",
    "\n",
    "## What You'll Learn:\n",
    "1. **Architecture** - How to stack layers\n",
    "2. **Implementation from scratch** in NumPy\n",
    "3. **Implementation in PyTorch**\n",
    "4. **Training on real datasets** (MNIST digits, XOR)\n",
    "5. **Best practices** for network design\n",
    "\n",
    "A feedforward network is called \"feedforward\" because information flows in one direction: input → hidden layers → output (no loops)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.datasets import make_moons, make_circles\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Network Architecture\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **Input Layer**: Receives raw data (not really a \"layer\", just inputs)\n",
    "2. **Hidden Layers**: Process and transform data (1 or more)\n",
    "3. **Output Layer**: Produces final predictions\n",
    "\n",
    "### Example: 784 → 128 → 64 → 10\n",
    "- Input: 784 features (28x28 image flattened)\n",
    "- Hidden 1: 128 neurons with ReLU\n",
    "- Hidden 2: 64 neurons with ReLU  \n",
    "- Output: 10 neurons (10 classes) with softmax\n",
    "\n",
    "### Universal Approximation Theorem:\n",
    "A feedforward network with a single hidden layer can approximate **any continuous function**, given enough neurons!\n",
    "\n",
    "(In practice, deeper networks work better than very wide shallow ones.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Helper Functions (NumPy Implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Activation functions\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-np.clip(z, -500, 500)))  # clip for numerical stability\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "def softmax(z):\n",
    "    \"\"\"Softmax activation (for multi-class output)\"\"\"\n",
    "    exp_z = np.exp(z - np.max(z, axis=0, keepdims=True))  # numerical stability\n",
    "    return exp_z / np.sum(exp_z, axis=0, keepdims=True)\n",
    "\n",
    "# Loss functions\n",
    "def mse_loss(y_true, y_pred):\n",
    "    \"\"\"Mean Squared Error\"\"\"\n",
    "    return np.mean((y_true - y_pred)**2)\n",
    "\n",
    "def binary_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"Binary Cross-Entropy\"\"\"\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    return -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "\n",
    "def categorical_cross_entropy(y_true, y_pred):\n",
    "    \"\"\"Categorical Cross-Entropy\"\"\"\n",
    "    y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
    "    return -np.mean(np.sum(y_true * np.log(y_pred), axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feedforward Network from Scratch (NumPy)\n",
    "\n",
    "We'll build a flexible class that can create networks with any architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    \"\"\"\n",
    "    A simple feedforward neural network implemented in NumPy\n",
    "    \n",
    "    Example:\n",
    "        nn = NeuralNetwork([2, 4, 3, 1])  # 2 inputs, 2 hidden layers (4 and 3 neurons), 1 output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, layer_sizes, activation='relu', output_activation='sigmoid'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            layer_sizes: list of layer sizes [input_size, hidden1, hidden2, ..., output_size]\n",
    "            activation: activation for hidden layers\n",
    "            output_activation: activation for output layer\n",
    "        \"\"\"\n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.num_layers = len(layer_sizes)\n",
    "        self.activation = activation\n",
    "        self.output_activation = output_activation\n",
    "        \n",
    "        # Initialize parameters\n",
    "        self.parameters = {}\n",
    "        for i in range(1, self.num_layers):\n",
    "            # He initialization for ReLU\n",
    "            self.parameters[f'W{i}'] = np.random.randn(layer_sizes[i], layer_sizes[i-1]) * np.sqrt(2.0 / layer_sizes[i-1])\n",
    "            self.parameters[f'b{i}'] = np.zeros((layer_sizes[i], 1))\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        \n",
    "        Args:\n",
    "            X: input data (features x samples)\n",
    "        \n",
    "        Returns:\n",
    "            cache: dictionary with all Z and A values\n",
    "        \"\"\"\n",
    "        cache = {'A0': X}\n",
    "        A = X\n",
    "        \n",
    "        # Forward through hidden layers\n",
    "        for i in range(1, self.num_layers - 1):\n",
    "            Z = self.parameters[f'W{i}'] @ A + self.parameters[f'b{i}']\n",
    "            A = relu(Z) if self.activation == 'relu' else sigmoid(Z)\n",
    "            cache[f'Z{i}'] = Z\n",
    "            cache[f'A{i}'] = A\n",
    "        \n",
    "        # Output layer\n",
    "        i = self.num_layers - 1\n",
    "        Z = self.parameters[f'W{i}'] @ A + self.parameters[f'b{i}']\n",
    "        \n",
    "        if self.output_activation == 'sigmoid':\n",
    "            A = sigmoid(Z)\n",
    "        elif self.output_activation == 'softmax':\n",
    "            A = softmax(Z)\n",
    "        else:\n",
    "            A = Z  # linear\n",
    "        \n",
    "        cache[f'Z{i}'] = Z\n",
    "        cache[f'A{i}'] = A\n",
    "        \n",
    "        return cache\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        cache = self.forward(X)\n",
    "        return cache[f'A{self.num_layers - 1}']\n",
    "    \n",
    "    def compute_loss(self, y_true, y_pred, loss_type='mse'):\n",
    "        \"\"\"Compute loss\"\"\"\n",
    "        if loss_type == 'mse':\n",
    "            return mse_loss(y_true, y_pred)\n",
    "        elif loss_type == 'binary_crossentropy':\n",
    "            return binary_cross_entropy(y_true, y_pred)\n",
    "        elif loss_type == 'categorical_crossentropy':\n",
    "            return categorical_cross_entropy(y_true, y_pred)\n",
    "\n",
    "# Test it\n",
    "nn = NeuralNetwork([2, 4, 3, 1])\n",
    "X_test = np.random.randn(2, 5)  # 2 features, 5 samples\n",
    "output = nn.predict(X_test)\n",
    "\n",
    "print(\"Network architecture:\", nn.layer_sizes)\n",
    "print(f\"Input shape: {X_test.shape}\")\n",
    "print(f\"Output shape: {output.shape}\")\n",
    "print(f\"Output values:\\n{output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the Network: Simple Gradient Descent Preview\n",
    "\n",
    "We'll implement a simple version here. Full backpropagation is in the next notebook!\n",
    "\n",
    "For now, we'll use **numerical gradients** (slow but educational)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numerical_gradient(nn, X, y, param_name, loss_type='mse', epsilon=1e-5):\n",
    "    \"\"\"\n",
    "    Compute gradient numerically (slow but useful for checking)\n",
    "    \"\"\"\n",
    "    original = nn.parameters[param_name].copy()\n",
    "    grad = np.zeros_like(original)\n",
    "    \n",
    "    it = np.nditer(original, flags=['multi_index'])\n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        \n",
    "        # Compute f(x + epsilon)\n",
    "        nn.parameters[param_name][idx] = original[idx] + epsilon\n",
    "        y_pred_plus = nn.predict(X)\n",
    "        loss_plus = nn.compute_loss(y, y_pred_plus, loss_type)\n",
    "        \n",
    "        # Compute f(x - epsilon)\n",
    "        nn.parameters[param_name][idx] = original[idx] - epsilon\n",
    "        y_pred_minus = nn.predict(X)\n",
    "        loss_minus = nn.compute_loss(y, y_pred_minus, loss_type)\n",
    "        \n",
    "        # Compute gradient\n",
    "        grad[idx] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "        \n",
    "        # Restore original value\n",
    "        nn.parameters[param_name][idx] = original[idx]\n",
    "        it.iternext()\n",
    "    \n",
    "    return grad\n",
    "\n",
    "print(\"Numerical gradient function ready (we'll use backprop in the next notebook!)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Solving XOR with a Feedforward Network\n",
    "\n",
    "XOR is a classic problem that's not linearly separable. A single neuron can't solve it, but a network with one hidden layer can!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0, 1, 1],\n",
    "                  [0, 1, 0, 1]])\n",
    "y_xor = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "print(\"XOR Truth Table:\")\n",
    "for i in range(4):\n",
    "    print(f\"  {X_xor[0,i]} XOR {X_xor[1,i]} = {y_xor[0,i]}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(6, 6))\n",
    "colors = ['red' if y == 0 else 'blue' for y in y_xor[0]]\n",
    "plt.scatter(X_xor[0], X_xor[1], c=colors, s=200, edgecolors='black', linewidth=2)\n",
    "plt.xlabel('Input 1')\n",
    "plt.ylabel('Input 2')\n",
    "plt.title('XOR Problem (Red=0, Blue=1)\\nNot linearly separable!')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlim(-0.5, 1.5)\n",
    "plt.ylim(-0.5, 1.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nNote: No single straight line can separate red from blue points!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. PyTorch Implementation\n",
    "\n",
    "Now let's build and train the same network in PyTorch (much easier!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedforwardNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Flexible feedforward neural network in PyTorch\n",
    "    \"\"\"\n",
    "    def __init__(self, layer_sizes, activation='relu', output_activation='sigmoid'):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.activation = activation\n",
    "        self.output_activation = output_activation\n",
    "        \n",
    "        # Create layers\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            self.layers.append(nn.Linear(layer_sizes[i], layer_sizes[i+1]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward through hidden layers\n",
    "        for layer in self.layers[:-1]:\n",
    "            x = layer(x)\n",
    "            if self.activation == 'relu':\n",
    "                x = torch.relu(x)\n",
    "            elif self.activation == 'sigmoid':\n",
    "                x = torch.sigmoid(x)\n",
    "            elif self.activation == 'tanh':\n",
    "                x = torch.tanh(x)\n",
    "        \n",
    "        # Output layer\n",
    "        x = self.layers[-1](x)\n",
    "        if self.output_activation == 'sigmoid':\n",
    "            x = torch.sigmoid(x)\n",
    "        elif self.output_activation == 'softmax':\n",
    "            x = torch.softmax(x, dim=1)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Create model for XOR\n",
    "model = FeedforwardNN([2, 8, 1], activation='relu', output_activation='sigmoid')\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Training Loop in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X_train = torch.tensor(X_xor.T, dtype=torch.float32)  # shape: (4, 2)\n",
    "y_train = torch.tensor(y_xor.T, dtype=torch.float32)  # shape: (4, 1)\n",
    "\n",
    "# Define loss and optimizer\n",
    "criterion = nn.BCELoss()  # Binary Cross-Entropy\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "num_epochs = 2000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred, y_train)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()  # Clear previous gradients\n",
    "    loss.backward()        # Compute gradients\n",
    "    optimizer.step()       # Update weights\n",
    "    \n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    if (epoch + 1) % 400 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Plot training loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the trained model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = model(X_train)\n",
    "    \n",
    "print(\"XOR Results After Training:\")\n",
    "print(\"Input\\t\\tTarget\\tPrediction\\tRounded\")\n",
    "print(\"-\" * 50)\n",
    "for i in range(4):\n",
    "    x1, x2 = X_train[i]\n",
    "    target = y_train[i].item()\n",
    "    pred = predictions[i].item()\n",
    "    rounded = round(pred)\n",
    "    print(f\"[{x1:.0f}, {x2:.0f}]\\t\\t{target:.0f}\\t{pred:.4f}\\t\\t{rounded}\")\n",
    "\n",
    "print(\"\\nSuccess! The network learned XOR!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. More Complex Dataset: Moons\n",
    "\n",
    "Let's try a more challenging non-linear dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate moons dataset\n",
    "X_moons, y_moons = make_moons(n_samples=1000, noise=0.1, random_state=42)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_moons, y_moons, test_size=0.2, random_state=42)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis', alpha=0.5)\n",
    "plt.colorbar(label='Class')\n",
    "plt.title('Moons Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32).unsqueeze(1)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32).unsqueeze(1)\n",
    "\n",
    "# Create and train model\n",
    "model_moons = FeedforwardNN([2, 16, 8, 1], activation='relu', output_activation='sigmoid')\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_moons.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "losses = []\n",
    "accuracies = []\n",
    "num_epochs = 1000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward pass\n",
    "    y_pred = model_moons(X_train_t)\n",
    "    loss = criterion(y_pred, y_train_t)\n",
    "    \n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Track metrics\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Compute accuracy\n",
    "    with torch.no_grad():\n",
    "        y_pred_class = (y_pred > 0.5).float()\n",
    "        accuracy = (y_pred_class == y_train_t).float().mean().item()\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    if (epoch + 1) % 200 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Plot metrics\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 4))\n",
    "\n",
    "ax1.plot(losses)\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "ax2.plot(accuracies)\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Accuracy')\n",
    "ax2.set_title('Training Accuracy')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "model_moons.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model_moons(X_test_t)\n",
    "    y_test_class = (y_test_pred > 0.5).float()\n",
    "    test_accuracy = (y_test_class == y_test_t).float().mean().item()\n",
    "\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Visualize Decision Boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_decision_boundary(model, X, y, title=\"Decision Boundary\"):\n",
    "    \"\"\"\n",
    "    Plot decision boundary for a binary classifier\n",
    "    \"\"\"\n",
    "    # Create mesh\n",
    "    h = 0.01\n",
    "    x_min, x_max = X[:, 0].min() - 0.5, X[:, 0].max() + 0.5\n",
    "    y_min, y_max = X[:, 1].min() - 0.5, X[:, 1].max() + 0.5\n",
    "    xx, yy = np.meshgrid(np.arange(x_min, x_max, h),\n",
    "                         np.arange(y_min, y_max, h))\n",
    "    \n",
    "    # Predict on mesh\n",
    "    mesh_input = torch.tensor(np.c_[xx.ravel(), yy.ravel()], dtype=torch.float32)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        Z = model(mesh_input).numpy()\n",
    "    Z = Z.reshape(xx.shape)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, levels=20, cmap='viridis')\n",
    "    plt.colorbar(label='Predicted Probability')\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', edgecolors='black', linewidth=0.5)\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.show()\n",
    "\n",
    "plot_decision_boundary(model_moons, X_test, y_test, \"Decision Boundary on Test Set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Multi-Class Classification\n",
    "\n",
    "Let's build a network that classifies into 3 or more classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate 3-class dataset\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "X_blobs, y_blobs = make_blobs(n_samples=600, centers=3, n_features=2, \n",
    "                              cluster_std=1.0, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_blobs, y_blobs, test_size=0.2, random_state=42)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, cmap='viridis')\n",
    "plt.colorbar(label='Class')\n",
    "plt.title('3-Class Dataset')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.long)  # Long for CrossEntropyLoss\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Create model - NOTE: No softmax in output (CrossEntropyLoss includes it)\n",
    "model_multi = FeedforwardNN([2, 16, 8, 3], activation='relu', output_activation='linear')\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()  # Combines softmax + cross-entropy\n",
    "optimizer = optim.Adam(model_multi.parameters(), lr=0.01)\n",
    "\n",
    "# Training\n",
    "losses = []\n",
    "accuracies = []\n",
    "num_epochs = 500\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Forward\n",
    "    y_pred = model_multi(X_train_t)\n",
    "    loss = criterion(y_pred, y_train_t)\n",
    "    \n",
    "    # Backward\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Metrics\n",
    "    losses.append(loss.item())\n",
    "    with torch.no_grad():\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        accuracy = (predicted == y_train_t).float().mean().item()\n",
    "        accuracies.append(accuracy)\n",
    "    \n",
    "    if (epoch + 1) % 100 == 0:\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item():.4f}, Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Evaluate on test\n",
    "model_multi.eval()\n",
    "with torch.no_grad():\n",
    "    y_test_pred = model_multi(X_test_t)\n",
    "    _, predicted = torch.max(y_test_pred, 1)\n",
    "    test_accuracy = (predicted == y_test_t).float().mean().item()\n",
    "\n",
    "print(f\"\\nTest Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Best Practices and Tips\n",
    "\n",
    "### Architecture Design:\n",
    "\n",
    "1. **Start Simple**: Begin with 1-2 hidden layers\n",
    "2. **Layer Sizes**: Typically decrease as you go deeper (e.g., 128 → 64 → 32)\n",
    "3. **Universal Rule**: More data → can support more parameters (larger/deeper networks)\n",
    "\n",
    "### Activation Functions:\n",
    "\n",
    "1. **Hidden Layers**: Use ReLU (default choice)\n",
    "2. **Output Layer**:\n",
    "   - Binary classification → Sigmoid\n",
    "   - Multi-class classification → Softmax (or linear with CrossEntropyLoss)\n",
    "   - Regression → Linear (no activation)\n",
    "\n",
    "### Initialization:\n",
    "\n",
    "1. **He Initialization** for ReLU: $W \\sim N(0, \\sqrt{2/n_{in}})$\n",
    "2. **Xavier Initialization** for tanh/sigmoid: $W \\sim N(0, \\sqrt{1/n_{in}})$\n",
    "3. **Biases**: Usually initialized to zero\n",
    "\n",
    "### Loss Functions:\n",
    "\n",
    "1. **Regression**: MSE, MAE\n",
    "2. **Binary Classification**: BCELoss\n",
    "3. **Multi-class**: CrossEntropyLoss\n",
    "\n",
    "### Common Issues:\n",
    "\n",
    "1. **Vanishing Gradients**: Use ReLU instead of sigmoid/tanh\n",
    "2. **Exploding Gradients**: Use gradient clipping, proper initialization\n",
    "3. **Overfitting**: Add dropout, L2 regularization, reduce model size\n",
    "4. **Underfitting**: Increase model size, train longer, reduce regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### What We Learned:\n",
    "\n",
    "1. **Feedforward Networks**:\n",
    "   - Stack multiple layers: input → hidden → ... → output\n",
    "   - Each layer: linear transform + activation\n",
    "   - Information flows forward only (no loops)\n",
    "\n",
    "2. **Implementation**:\n",
    "   - Built from scratch in NumPy\n",
    "   - Much simpler in PyTorch\n",
    "   - Forward pass computes predictions\n",
    "\n",
    "3. **Training**:\n",
    "   - Define loss function\n",
    "   - Compute gradients (backprop)\n",
    "   - Update weights with optimizer\n",
    "   - Repeat!\n",
    "\n",
    "4. **Applications**:\n",
    "   - Binary classification (XOR, moons)\n",
    "   - Multi-class classification (3 classes)\n",
    "   - Works for non-linear decision boundaries\n",
    "\n",
    "### Next Steps:\n",
    "In the next notebook, we'll dive deep into **backpropagation** - how gradients are actually computed!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Build a 3-layer network (input→16→8→output) for the circles dataset\n",
    "X_circles, y_circles = make_circles(n_samples=1000, noise=0.05, factor=0.5, random_state=42)\n",
    "\n",
    "# Visualize the dataset\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.scatter(X_circles[:, 0], X_circles[:, 1], c=y_circles, cmap='viridis')\n",
    "plt.title('Circles Dataset - Your Turn!')\n",
    "plt.show()\n",
    "\n",
    "# Your code here:\n",
    "# 1. Convert to PyTorch tensors\n",
    "# 2. Create model\n",
    "# 3. Train for 1000 epochs\n",
    "# 4. Evaluate accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Experiment with different architectures\n",
    "# Try these and compare performance:\n",
    "# - Shallow and wide: [2, 64, 1]\n",
    "# - Deep and narrow: [2, 8, 8, 8, 1]\n",
    "# - Balanced: [2, 16, 16, 1]\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Implement a regression network\n",
    "# Create a dataset with y = x^2 + noise\n",
    "# Build a network to learn this function\n",
    "X_reg = np.random.randn(1000, 1) * 2\n",
    "y_reg = X_reg**2 + np.random.randn(1000, 1) * 0.1\n",
    "\n",
    "# Your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
