{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Math Foundations for Neural Networks\n",
    "\n",
    "This notebook covers the essential mathematics needed to understand neural networks:\n",
    "1. **Linear Algebra** - Vectors, matrices, and operations\n",
    "2. **Calculus** - Derivatives, partial derivatives, and the chain rule\n",
    "3. **Probability Basics** - Distributions and expectations\n",
    "\n",
    "We'll use NumPy to demonstrate these concepts with code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Linear Algebra Fundamentals\n",
    "\n",
    "Neural networks are built on linear algebra operations. Understanding vectors and matrices is crucial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Vectors\n",
    "\n",
    "A vector is an ordered list of numbers. In neural networks, vectors represent:\n",
    "- Input features (e.g., pixel values, word embeddings)\n",
    "- Neuron activations\n",
    "- Weight updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating vectors in NumPy\n",
    "v1 = np.array([1, 2, 3])\n",
    "v2 = np.array([4, 5, 6])\n",
    "\n",
    "print(f\"Vector v1: {v1}\")\n",
    "print(f\"Vector v2: {v2}\")\n",
    "print(f\"Shape: {v1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Vector Operations\n",
    "\n",
    "**Element-wise addition:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise addition\n",
    "v_add = v1 + v2\n",
    "print(f\"v1 + v2 = {v_add}\")\n",
    "\n",
    "# Element-wise multiplication (Hadamard product)\n",
    "v_mult = v1 * v2\n",
    "print(f\"v1 * v2 (element-wise) = {v_mult}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Dot Product (Inner Product)\n",
    "\n",
    "The dot product is fundamental to neural networks. It's used in:\n",
    "- Computing neuron activations: $z = w \\cdot x + b$\n",
    "- Attention mechanisms\n",
    "- Similarity measures\n",
    "\n",
    "**Formula:** $v_1 \\cdot v_2 = \\sum_{i=1}^{n} v_{1i} \\cdot v_{2i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dot product\n",
    "dot_product = np.dot(v1, v2)\n",
    "print(f\"v1 · v2 = {dot_product}\")\n",
    "print(f\"Manual calculation: {v1[0]*v2[0]} + {v1[1]*v2[1]} + {v1[2]*v2[2]} = {dot_product}\")\n",
    "\n",
    "# Alternative notation\n",
    "dot_product_alt = v1 @ v2\n",
    "print(f\"Using @ operator: {dot_product_alt}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Matrices\n",
    "\n",
    "Matrices are 2D arrays of numbers. In neural networks:\n",
    "- **Weight matrices** connect layers\n",
    "- **Data matrices** store batches of examples\n",
    "- **Gradient matrices** store derivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating matrices\n",
    "M1 = np.array([[1, 2, 3],\n",
    "               [4, 5, 6]])\n",
    "\n",
    "M2 = np.array([[7, 8],\n",
    "               [9, 10],\n",
    "               [11, 12]])\n",
    "\n",
    "print(\"Matrix M1 (2x3):\")\n",
    "print(M1)\n",
    "print(f\"Shape: {M1.shape}\\n\")\n",
    "\n",
    "print(\"Matrix M2 (3x2):\")\n",
    "print(M2)\n",
    "print(f\"Shape: {M2.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Matrix Multiplication\n",
    "\n",
    "Matrix multiplication is the core operation in neural networks!\n",
    "\n",
    "**Rule:** For matrices $A_{m \\times n}$ and $B_{n \\times p}$, the product $C = AB$ has shape $m \\times p$\n",
    "\n",
    "Each element: $C_{ij} = \\sum_{k=1}^{n} A_{ik} \\cdot B_{kj}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication\n",
    "M3 = M1 @ M2  # or np.dot(M1, M2)\n",
    "\n",
    "print(f\"M1 @ M2:\")\n",
    "print(M3)\n",
    "print(f\"Shape: {M3.shape}\")\n",
    "\n",
    "# Verify dimensions work\n",
    "print(f\"\\nDimension check: ({M1.shape[0]}x{M1.shape[1]}) @ ({M2.shape[0]}x{M2.shape[1]}) = ({M3.shape[0]}x{M3.shape[1]})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 Matrix-Vector Multiplication (Neural Network Layer)\n",
    "\n",
    "This is exactly how a neural network layer computes its output:\n",
    "\n",
    "$y = Wx + b$\n",
    "\n",
    "Where:\n",
    "- $W$ is the weight matrix\n",
    "- $x$ is the input vector\n",
    "- $b$ is the bias vector\n",
    "- $y$ is the output vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: 3 inputs, 2 neurons\n",
    "W = np.array([[0.5, 0.2, 0.1],   # weights for neuron 1\n",
    "              [0.3, 0.4, 0.6]])  # weights for neuron 2\n",
    "\n",
    "x = np.array([1.0, 2.0, 3.0])    # input features\n",
    "b = np.array([0.1, 0.2])         # biases\n",
    "\n",
    "# Compute layer output\n",
    "y = W @ x + b\n",
    "\n",
    "print(f\"Input x: {x}\")\n",
    "print(f\"Weight matrix W:\\n{W}\")\n",
    "print(f\"Bias b: {b}\")\n",
    "print(f\"\\nOutput y = Wx + b: {y}\")\n",
    "print(f\"\\nThis is a 2-neuron layer processing 3 input features!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 Transpose\n",
    "\n",
    "The transpose operation flips rows and columns. Critical for backpropagation!\n",
    "\n",
    "Notation: $A^T$ or $A'$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2, 3],\n",
    "              [4, 5, 6]])\n",
    "\n",
    "A_T = A.T\n",
    "\n",
    "print(f\"Original A ({A.shape}):\")\n",
    "print(A)\n",
    "print(f\"\\nTranspose A^T ({A_T.shape}):\")\n",
    "print(A_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Calculus for Neural Networks\n",
    "\n",
    "Calculus helps us understand how small changes in inputs affect outputs. This is essential for:\n",
    "- **Training neural networks** (gradient descent)\n",
    "- **Backpropagation** (computing gradients)\n",
    "- **Optimization** (finding minima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Derivatives\n",
    "\n",
    "The derivative measures how a function changes as its input changes.\n",
    "\n",
    "**Definition:** $f'(x) = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h}$\n",
    "\n",
    "**Geometric interpretation:** Slope of the tangent line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: f(x) = x^2\n",
    "# Derivative: f'(x) = 2x\n",
    "\n",
    "def f(x):\n",
    "    return x**2\n",
    "\n",
    "def f_prime(x):\n",
    "    return 2*x\n",
    "\n",
    "# Numerical approximation of derivative\n",
    "def numerical_derivative(f, x, h=1e-5):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "x = 3.0\n",
    "print(f\"Function value at x={x}: f({x}) = {f(x)}\")\n",
    "print(f\"Analytical derivative: f'({x}) = {f_prime(x)}\")\n",
    "print(f\"Numerical derivative: f'({x}) ≈ {numerical_derivative(f, x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the function and its derivative\n",
    "x_vals = np.linspace(-5, 5, 100)\n",
    "y_vals = f(x_vals)\n",
    "dy_vals = f_prime(x_vals)\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x_vals, y_vals, label='f(x) = x²', linewidth=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('f(x)')\n",
    "plt.title('Function f(x) = x²')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_vals, dy_vals, label=\"f'(x) = 2x\", color='orange', linewidth=2)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.xlabel('x')\n",
    "plt.ylabel(\"f'(x)\")\n",
    "plt.title('Derivative f\\'(x) = 2x')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Common Derivatives (Reference)\n",
    "\n",
    "These appear constantly in neural networks:\n",
    "\n",
    "| Function | Derivative |\n",
    "|----------|------------|\n",
    "| $f(x) = c$ (constant) | $f'(x) = 0$ |\n",
    "| $f(x) = x$ | $f'(x) = 1$ |\n",
    "| $f(x) = x^n$ | $f'(x) = nx^{n-1}$ |\n",
    "| $f(x) = e^x$ | $f'(x) = e^x$ |\n",
    "| $f(x) = \\ln(x)$ | $f'(x) = \\frac{1}{x}$ |\n",
    "| $f(x) = \\sin(x)$ | $f'(x) = \\cos(x)$ |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 The Chain Rule (Most Important for Backpropagation!)\n",
    "\n",
    "The chain rule tells us how to differentiate composite functions:\n",
    "\n",
    "**If** $y = f(g(x))$, **then** $\\frac{dy}{dx} = \\frac{df}{dg} \\cdot \\frac{dg}{dx}$\n",
    "\n",
    "This is the mathematical foundation of backpropagation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: y = (2x + 1)^3\n",
    "# Let g(x) = 2x + 1, so f(g) = g^3\n",
    "# dy/dx = dy/dg * dg/dx = 3g^2 * 2 = 6(2x + 1)^2\n",
    "\n",
    "def composite_function(x):\n",
    "    \"\"\"y = (2x + 1)^3\"\"\"\n",
    "    return (2*x + 1)**3\n",
    "\n",
    "def composite_derivative(x):\n",
    "    \"\"\"dy/dx = 6(2x + 1)^2\"\"\"\n",
    "    return 6 * (2*x + 1)**2\n",
    "\n",
    "x = 2.0\n",
    "print(f\"Analytical derivative at x={x}: {composite_derivative(x)}\")\n",
    "print(f\"Numerical derivative at x={x}: {numerical_derivative(composite_function, x)}\")\n",
    "\n",
    "# Breaking it down with chain rule\n",
    "g = 2*x + 1  # inner function\n",
    "dg_dx = 2    # derivative of inner function\n",
    "df_dg = 3 * g**2  # derivative of outer function\n",
    "dy_dx = df_dg * dg_dx  # chain rule\n",
    "\n",
    "print(f\"\\nChain rule breakdown:\")\n",
    "print(f\"  g = 2x + 1 = {g}\")\n",
    "print(f\"  dg/dx = {dg_dx}\")\n",
    "print(f\"  df/dg = 3g² = {df_dg}\")\n",
    "print(f\"  dy/dx = df/dg * dg/dx = {dy_dx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 Partial Derivatives\n",
    "\n",
    "When a function has multiple inputs, we compute the **partial derivative** with respect to each input, treating others as constants.\n",
    "\n",
    "**Example:** $f(x, y) = x^2 + xy + y^2$\n",
    "\n",
    "- $\\frac{\\partial f}{\\partial x} = 2x + y$\n",
    "- $\\frac{\\partial f}{\\partial y} = x + 2y$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_xy(x, y):\n",
    "    \"\"\"f(x,y) = x^2 + xy + y^2\"\"\"\n",
    "    return x**2 + x*y + y**2\n",
    "\n",
    "def df_dx(x, y):\n",
    "    \"\"\"∂f/∂x = 2x + y\"\"\"\n",
    "    return 2*x + y\n",
    "\n",
    "def df_dy(x, y):\n",
    "    \"\"\"∂f/∂y = x + 2y\"\"\"\n",
    "    return x + 2*y\n",
    "\n",
    "x, y = 3.0, 4.0\n",
    "print(f\"f({x}, {y}) = {f_xy(x, y)}\")\n",
    "print(f\"\\nPartial derivatives:\")\n",
    "print(f\"  ∂f/∂x = {df_dx(x, y)}\")\n",
    "print(f\"  ∂f/∂y = {df_dy(x, y)}\")\n",
    "\n",
    "# Numerical verification\n",
    "h = 1e-5\n",
    "numerical_dx = (f_xy(x+h, y) - f_xy(x, y)) / h\n",
    "numerical_dy = (f_xy(x, y+h) - f_xy(x, y)) / h\n",
    "\n",
    "print(f\"\\nNumerical verification:\")\n",
    "print(f\"  ∂f/∂x ≈ {numerical_dx}\")\n",
    "print(f\"  ∂f/∂y ≈ {numerical_dy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 Gradients\n",
    "\n",
    "The **gradient** is a vector of all partial derivatives:\n",
    "\n",
    "$\\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix}$\n",
    "\n",
    "The gradient points in the direction of steepest increase!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient of f(x,y) = x^2 + xy + y^2\n",
    "def gradient(x, y):\n",
    "    \"\"\"Returns gradient vector [∂f/∂x, ∂f/∂y]\"\"\"\n",
    "    return np.array([df_dx(x, y), df_dy(x, y)])\n",
    "\n",
    "grad = gradient(3.0, 4.0)\n",
    "print(f\"Gradient at (3, 4): {grad}\")\n",
    "print(f\"This vector points in the direction of steepest increase!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the gradient field\n",
    "x = np.linspace(-3, 3, 20)\n",
    "y = np.linspace(-3, 3, 20)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "# Compute function values\n",
    "Z = f_xy(X, Y)\n",
    "\n",
    "# Compute gradient at each point\n",
    "U = df_dx(X, Y)  # ∂f/∂x\n",
    "V = df_dy(X, Y)  # ∂f/∂y\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "contour = plt.contour(X, Y, Z, levels=20)\n",
    "plt.clabel(contour, inline=True, fontsize=8)\n",
    "plt.title('Function f(x,y) = x² + xy + y²')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.contour(X, Y, Z, levels=20, alpha=0.3)\n",
    "plt.quiver(X, Y, U, V, alpha=0.6)\n",
    "plt.title('Gradient Field (arrows point uphill)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Notice: Gradients point away from the minimum at (0,0) toward higher values!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Probability Basics\n",
    "\n",
    "Neural networks use probability for:\n",
    "- **Classification** (softmax outputs are probabilities)\n",
    "- **Loss functions** (cross-entropy)\n",
    "- **Regularization** (dropout)\n",
    "- **Uncertainty estimation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Probability Distributions\n",
    "\n",
    "A probability distribution assigns probabilities to different outcomes.\n",
    "\n",
    "**Key properties:**\n",
    "- All probabilities are between 0 and 1: $0 \\leq P(x) \\leq 1$\n",
    "- All probabilities sum to 1: $\\sum P(x) = 1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Probability distribution over 3 classes\n",
    "class_probs = np.array([0.7, 0.2, 0.1])\n",
    "\n",
    "print(f\"Class probabilities: {class_probs}\")\n",
    "print(f\"Sum of probabilities: {class_probs.sum()} (must equal 1)\")\n",
    "print(f\"All between 0 and 1: {np.all((class_probs >= 0) & (class_probs <= 1))}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.bar(['Class 0', 'Class 1', 'Class 2'], class_probs)\n",
    "plt.ylabel('Probability')\n",
    "plt.title('Probability Distribution Over Classes')\n",
    "plt.ylim(0, 1)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Expected Value (Mean)\n",
    "\n",
    "The expected value is the average outcome weighted by probability:\n",
    "\n",
    "$E[X] = \\sum_{i} x_i \\cdot P(x_i)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Expected value of a dice roll\n",
    "outcomes = np.array([1, 2, 3, 4, 5, 6])\n",
    "probabilities = np.array([1/6, 1/6, 1/6, 1/6, 1/6, 1/6])\n",
    "\n",
    "expected_value = np.sum(outcomes * probabilities)\n",
    "print(f\"Expected value of fair dice: {expected_value}\")\n",
    "\n",
    "# Verify with simulation\n",
    "rolls = np.random.randint(1, 7, size=10000)\n",
    "print(f\"Average from 10,000 simulated rolls: {rolls.mean():.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Softmax Function (Converting to Probabilities)\n",
    "\n",
    "Neural network outputs are often arbitrary real numbers (logits). Softmax converts them to probabilities:\n",
    "\n",
    "$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j} e^{z_j}}$\n",
    "\n",
    "This ensures:\n",
    "- All outputs are positive\n",
    "- All outputs sum to 1\n",
    "- Maintains relative ordering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    \"\"\"Compute softmax values for array z\"\"\"\n",
    "    exp_z = np.exp(z - np.max(z))  # subtract max for numerical stability\n",
    "    return exp_z / exp_z.sum()\n",
    "\n",
    "# Example: Neural network outputs (logits)\n",
    "logits = np.array([2.0, 1.0, 0.1])\n",
    "\n",
    "probs = softmax(logits)\n",
    "\n",
    "print(f\"Raw logits: {logits}\")\n",
    "print(f\"After softmax: {probs}\")\n",
    "print(f\"Sum of probabilities: {probs.sum()}\")\n",
    "\n",
    "# Visualize\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "ax1.bar(['Class 0', 'Class 1', 'Class 2'], logits)\n",
    "ax1.set_ylabel('Value')\n",
    "ax1.set_title('Raw Logits (network outputs)')\n",
    "ax1.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "ax2.bar(['Class 0', 'Class 1', 'Class 2'], probs)\n",
    "ax2.set_ylabel('Probability')\n",
    "ax2.set_title('After Softmax')\n",
    "ax2.set_ylim(0, 1)\n",
    "ax2.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You now have the mathematical foundation for neural networks!\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "**Linear Algebra:**\n",
    "- Vectors and matrices represent data and parameters\n",
    "- Matrix multiplication computes layer outputs: $y = Wx + b$\n",
    "- Transpose is crucial for backpropagation\n",
    "\n",
    "**Calculus:**\n",
    "- Derivatives measure sensitivity to changes\n",
    "- **Chain rule** is the foundation of backpropagation\n",
    "- Gradients point in the direction of steepest increase\n",
    "- We use negative gradients for gradient descent (going downhill)\n",
    "\n",
    "**Probability:**\n",
    "- Softmax converts outputs to probabilities\n",
    "- Expected value computes weighted averages\n",
    "- Probability distributions must sum to 1\n",
    "\n",
    "### Next Steps:\n",
    "In the next notebook, we'll use these concepts to build our first neural network!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises\n",
    "\n",
    "Try these exercises to reinforce your understanding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Compute the dot product of these vectors\n",
    "a = np.array([1, 2, 3, 4])\n",
    "b = np.array([5, 6, 7, 8])\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Create a 3x4 weight matrix and multiply it with a 4-element input vector\n",
    "# Then add a bias vector of size 3\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Implement a function to compute the numerical derivative\n",
    "# of f(x) = x^3 - 2x^2 + 1 at x = 2\n",
    "# Then compare with the analytical derivative: f'(x) = 3x^2 - 4x\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 4: Apply softmax to these logits and verify they sum to 1\n",
    "logits = np.array([3.2, 1.3, 0.2, 0.8])\n",
    "# Your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
