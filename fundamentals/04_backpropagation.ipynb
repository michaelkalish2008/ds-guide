{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation: The Heart of Deep Learning\n",
    "\n",
    "Backpropagation is the algorithm that makes training neural networks possible. It efficiently computes gradients using the chain rule.\n",
    "\n",
    "## What You'll Learn:\n",
    "1. **The Chain Rule** - Mathematical foundation\n",
    "2. **Computational Graphs** - Visual representation\n",
    "3. **Forward and Backward Pass** - The two-phase algorithm\n",
    "4. **Backprop from Scratch** - Complete NumPy implementation\n",
    "5. **PyTorch Autograd** - Automatic differentiation\n",
    "6. **Gradient Checking** - Verifying correctness\n",
    "\n",
    "**Key Insight:** Backpropagation is just the chain rule applied systematically to compute derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Review: The Chain Rule\n",
    "\n",
    "The chain rule tells us how to differentiate composite functions.\n",
    "\n",
    "### Single Variable:\n",
    "If $y = f(g(x))$, then:\n",
    "\n",
    "$\\frac{dy}{dx} = \\frac{dy}{dg} \\cdot \\frac{dg}{dx}$\n",
    "\n",
    "### Multiple Variables:\n",
    "If $z = f(x, y)$ where $x = g(t)$ and $y = h(t)$, then:\n",
    "\n",
    "$\\frac{dz}{dt} = \\frac{\\partial z}{\\partial x} \\cdot \\frac{dx}{dt} + \\frac{\\partial z}{\\partial y} \\cdot \\frac{dy}{dt}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Chain rule in action\n",
    "# Let's compute the derivative of f(x) = (2x + 1)^3\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"f(x) = (2x + 1)^3\"\"\"\n",
    "    return (2*x + 1)**3\n",
    "\n",
    "def df_dx_analytical(x):\n",
    "    \"\"\"\n",
    "    Analytical derivative using chain rule:\n",
    "    Let u = 2x + 1, so f = u^3\n",
    "    df/dx = df/du * du/dx = 3u^2 * 2 = 6(2x + 1)^2\n",
    "    \"\"\"\n",
    "    return 6 * (2*x + 1)**2\n",
    "\n",
    "# Test at x = 2\n",
    "x = 2.0\n",
    "print(f\"f({x}) = {f(x)}\")\n",
    "print(f\"f'({x}) = {df_dx_analytical(x)}\")\n",
    "\n",
    "# Break down the chain rule step by step\n",
    "u = 2*x + 1         # inner function\n",
    "du_dx = 2           # derivative of inner function\n",
    "df_du = 3 * u**2    # derivative of outer function\n",
    "df_dx = df_du * du_dx  # chain rule!\n",
    "\n",
    "print(f\"\\nChain rule breakdown:\")\n",
    "print(f\"  u = 2x + 1 = {u}\")\n",
    "print(f\"  du/dx = {du_dx}\")\n",
    "print(f\"  df/du = 3u² = {df_du}\")\n",
    "print(f\"  df/dx = df/du × du/dx = {df_dx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Computational Graphs\n",
    "\n",
    "A **computational graph** represents mathematical operations as a directed graph:\n",
    "- **Nodes**: Variables or operations\n",
    "- **Edges**: Flow of values\n",
    "\n",
    "### Example: $L = (w \\cdot x + b)^2$\n",
    "\n",
    "```\n",
    "w, x, b (inputs)\n",
    "   ↓\n",
    "  mul: a = w * x\n",
    "   ↓\n",
    "  add: z = a + b\n",
    "   ↓\n",
    " square: L = z^2\n",
    "```\n",
    "\n",
    "**Forward pass**: Compute values left to right  \n",
    "**Backward pass**: Compute gradients right to left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Simple computational graph\n",
    "# L = (w * x + b)^2\n",
    "\n",
    "# Inputs\n",
    "w = 2.0\n",
    "x = 3.0\n",
    "b = 1.0\n",
    "\n",
    "# Forward pass\n",
    "a = w * x        # multiply\n",
    "z = a + b        # add\n",
    "L = z ** 2       # square\n",
    "\n",
    "print(\"Forward Pass:\")\n",
    "print(f\"  a = w * x = {w} * {x} = {a}\")\n",
    "print(f\"  z = a + b = {a} + {b} = {z}\")\n",
    "print(f\"  L = z^2 = {z}^2 = {L}\")\n",
    "\n",
    "# Backward pass (computing gradients)\n",
    "# We want: dL/dw, dL/dx, dL/db\n",
    "\n",
    "# Start from the end: dL/dL = 1\n",
    "dL_dL = 1.0\n",
    "\n",
    "# dL/dz = dL/dL * dL/dz = 1 * 2z = 2z\n",
    "dL_dz = dL_dL * 2 * z\n",
    "\n",
    "# dL/da = dL/dz * dz/da = dL/dz * 1\n",
    "dL_da = dL_dz * 1\n",
    "\n",
    "# dL/db = dL/dz * dz/db = dL/dz * 1\n",
    "dL_db = dL_dz * 1\n",
    "\n",
    "# dL/dw = dL/da * da/dw = dL/da * x\n",
    "dL_dw = dL_da * x\n",
    "\n",
    "# dL/dx = dL/da * da/dx = dL/da * w\n",
    "dL_dx = dL_da * w\n",
    "\n",
    "print(\"\\nBackward Pass (Gradients):\")\n",
    "print(f\"  dL/dz = 2z = {dL_dz}\")\n",
    "print(f\"  dL/db = {dL_db}\")\n",
    "print(f\"  dL/dw = dL/da * x = {dL_dw}\")\n",
    "print(f\"  dL/dx = dL/da * w = {dL_dx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Backpropagation for a Single Neuron\n",
    "\n",
    "Let's work through backprop for a single neuron step by step.\n",
    "\n",
    "### Forward Pass:\n",
    "1. $z = w^T x + b$ (linear combination)\n",
    "2. $a = \\sigma(z)$ (activation)\n",
    "3. $L = \\frac{1}{2}(y - a)^2$ (loss)\n",
    "\n",
    "### Backward Pass (compute $\\frac{\\partial L}{\\partial w}$ and $\\frac{\\partial L}{\\partial b}$):\n",
    "\n",
    "Using the chain rule:\n",
    "\n",
    "$\\frac{\\partial L}{\\partial w} = \\frac{\\partial L}{\\partial a} \\cdot \\frac{\\partial a}{\\partial z} \\cdot \\frac{\\partial z}{\\partial w}$\n",
    "\n",
    "Let's compute each piece:\n",
    "1. $\\frac{\\partial L}{\\partial a} = a - y$\n",
    "2. $\\frac{\\partial a}{\\partial z} = \\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$\n",
    "3. $\\frac{\\partial z}{\\partial w} = x$\n",
    "\n",
    "Therefore:\n",
    "$\\frac{\\partial L}{\\partial w} = (a - y) \\cdot \\sigma'(z) \\cdot x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "def sigmoid_derivative(z):\n",
    "    s = sigmoid(z)\n",
    "    return s * (1 - s)\n",
    "\n",
    "# Example: Single neuron backprop\n",
    "x = np.array([1.0, 2.0, 3.0])  # input\n",
    "w = np.array([0.5, -0.2, 0.1]) # weights\n",
    "b = 0.3                         # bias\n",
    "y = 1.0                         # target\n",
    "\n",
    "# Forward pass\n",
    "z = np.dot(w, x) + b\n",
    "a = sigmoid(z)\n",
    "L = 0.5 * (y - a)**2\n",
    "\n",
    "print(\"Forward Pass:\")\n",
    "print(f\"  z = w·x + b = {z:.4f}\")\n",
    "print(f\"  a = σ(z) = {a:.4f}\")\n",
    "print(f\"  L = ½(y-a)² = {L:.4f}\")\n",
    "\n",
    "# Backward pass\n",
    "dL_da = a - y                    # ∂L/∂a\n",
    "da_dz = sigmoid_derivative(z)    # ∂a/∂z\n",
    "dz_dw = x                        # ∂z/∂w\n",
    "dz_db = 1.0                      # ∂z/∂b\n",
    "\n",
    "# Chain rule\n",
    "dL_dz = dL_da * da_dz            # ∂L/∂z\n",
    "dL_dw = dL_dz * dz_dw            # ∂L/∂w\n",
    "dL_db = dL_dz * dz_db            # ∂L/∂b\n",
    "\n",
    "print(\"\\nBackward Pass:\")\n",
    "print(f\"  ∂L/∂a = a - y = {dL_da:.4f}\")\n",
    "print(f\"  ∂a/∂z = σ'(z) = {da_dz:.4f}\")\n",
    "print(f\"  ∂L/∂z = ∂L/∂a × ∂a/∂z = {dL_dz:.4f}\")\n",
    "print(f\"  ∂L/∂w = {dL_dw}\")\n",
    "print(f\"  ∂L/∂b = {dL_db:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Backpropagation for Multi-Layer Networks\n",
    "\n",
    "For a 2-layer network: $X \\to Z^{[1]} \\to A^{[1]} \\to Z^{[2]} \\to A^{[2]} \\to L$\n",
    "\n",
    "### Forward Pass:\n",
    "1. $Z^{[1]} = W^{[1]}X + b^{[1]}$\n",
    "2. $A^{[1]} = \\sigma(Z^{[1]})$\n",
    "3. $Z^{[2]} = W^{[2]}A^{[1]} + b^{[2]}$\n",
    "4. $A^{[2]} = \\sigma(Z^{[2]})$\n",
    "5. $L = \\text{loss}(Y, A^{[2]})$\n",
    "\n",
    "### Backward Pass:\n",
    "\n",
    "Work backwards from the loss:\n",
    "\n",
    "**Layer 2 (Output):**\n",
    "1. $dZ^{[2]} = A^{[2]} - Y$ (for MSE loss)\n",
    "2. $dW^{[2]} = \\frac{1}{m} dZ^{[2]} (A^{[1]})^T$\n",
    "3. $db^{[2]} = \\frac{1}{m} \\sum dZ^{[2]}$\n",
    "\n",
    "**Layer 1 (Hidden):**\n",
    "1. $dA^{[1]} = (W^{[2]})^T dZ^{[2]}$\n",
    "2. $dZ^{[1]} = dA^{[1]} \\odot \\sigma'(Z^{[1]})$\n",
    "3. $dW^{[1]} = \\frac{1}{m} dZ^{[1]} X^T$\n",
    "4. $db^{[1]} = \\frac{1}{m} \\sum dZ^{[1]}$\n",
    "\n",
    "where $\\odot$ is element-wise multiplication."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Complete Implementation from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetworkWithBackprop:\n",
    "    \"\"\"\n",
    "    Simple 2-layer neural network with backpropagation\n",
    "    Architecture: input → hidden → output\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, output_size, learning_rate=0.01):\n",
    "        # Initialize weights with small random values\n",
    "        self.W1 = np.random.randn(hidden_size, input_size) * 0.1\n",
    "        self.b1 = np.zeros((hidden_size, 1))\n",
    "        self.W2 = np.random.randn(output_size, hidden_size) * 0.1\n",
    "        self.b2 = np.zeros((output_size, 1))\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def relu(self, Z):\n",
    "        return np.maximum(0, Z)\n",
    "    \n",
    "    def relu_derivative(self, Z):\n",
    "        return (Z > 0).astype(float)\n",
    "    \n",
    "    def sigmoid(self, Z):\n",
    "        return 1 / (1 + np.exp(-np.clip(Z, -500, 500)))\n",
    "    \n",
    "    def sigmoid_derivative(self, Z):\n",
    "        s = self.sigmoid(Z)\n",
    "        return s * (1 - s)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Forward propagation\n",
    "        \n",
    "        Args:\n",
    "            X: input (input_size x m)\n",
    "        \n",
    "        Returns:\n",
    "            cache: dict with intermediate values\n",
    "        \"\"\"\n",
    "        # Layer 1\n",
    "        Z1 = self.W1 @ X + self.b1\n",
    "        A1 = self.relu(Z1)\n",
    "        \n",
    "        # Layer 2 (output)\n",
    "        Z2 = self.W2 @ A1 + self.b2\n",
    "        A2 = self.sigmoid(Z2)\n",
    "        \n",
    "        cache = {\n",
    "            'X': X,\n",
    "            'Z1': Z1,\n",
    "            'A1': A1,\n",
    "            'Z2': Z2,\n",
    "            'A2': A2\n",
    "        }\n",
    "        \n",
    "        return A2, cache\n",
    "    \n",
    "    def compute_loss(self, Y, A2):\n",
    "        \"\"\"Binary cross-entropy loss\"\"\"\n",
    "        m = Y.shape[1]\n",
    "        A2 = np.clip(A2, 1e-15, 1 - 1e-15)  # avoid log(0)\n",
    "        loss = -np.sum(Y * np.log(A2) + (1 - Y) * np.log(1 - A2)) / m\n",
    "        return loss\n",
    "    \n",
    "    def backward(self, Y, cache):\n",
    "        \"\"\"\n",
    "        Backpropagation\n",
    "        \n",
    "        Args:\n",
    "            Y: true labels (output_size x m)\n",
    "            cache: dict from forward pass\n",
    "        \n",
    "        Returns:\n",
    "            gradients: dict with all gradients\n",
    "        \"\"\"\n",
    "        m = Y.shape[1]\n",
    "        X = cache['X']\n",
    "        Z1 = cache['Z1']\n",
    "        A1 = cache['A1']\n",
    "        Z2 = cache['Z2']\n",
    "        A2 = cache['A2']\n",
    "        \n",
    "        # Backward through layer 2\n",
    "        dZ2 = A2 - Y  # derivative of binary cross-entropy + sigmoid\n",
    "        dW2 = (1/m) * (dZ2 @ A1.T)\n",
    "        db2 = (1/m) * np.sum(dZ2, axis=1, keepdims=True)\n",
    "        \n",
    "        # Backward through layer 1\n",
    "        dA1 = self.W2.T @ dZ2\n",
    "        dZ1 = dA1 * self.relu_derivative(Z1)\n",
    "        dW1 = (1/m) * (dZ1 @ X.T)\n",
    "        db1 = (1/m) * np.sum(dZ1, axis=1, keepdims=True)\n",
    "        \n",
    "        gradients = {\n",
    "            'dW1': dW1,\n",
    "            'db1': db1,\n",
    "            'dW2': dW2,\n",
    "            'db2': db2\n",
    "        }\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def update_parameters(self, gradients):\n",
    "        \"\"\"Gradient descent update\"\"\"\n",
    "        self.W1 -= self.learning_rate * gradients['dW1']\n",
    "        self.b1 -= self.learning_rate * gradients['db1']\n",
    "        self.W2 -= self.learning_rate * gradients['dW2']\n",
    "        self.b2 -= self.learning_rate * gradients['db2']\n",
    "    \n",
    "    def train(self, X, Y, num_iterations=1000, print_every=100):\n",
    "        \"\"\"Complete training loop\"\"\"\n",
    "        losses = []\n",
    "        \n",
    "        for i in range(num_iterations):\n",
    "            # Forward pass\n",
    "            A2, cache = self.forward(X)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = self.compute_loss(Y, A2)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            # Backward pass\n",
    "            gradients = self.backward(Y, cache)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.update_parameters(gradients)\n",
    "            \n",
    "            # Print progress\n",
    "            if (i + 1) % print_every == 0:\n",
    "                print(f\"Iteration {i+1}/{num_iterations}, Loss: {loss:.4f}\")\n",
    "        \n",
    "        return losses\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions\"\"\"\n",
    "        A2, _ = self.forward(X)\n",
    "        return (A2 > 0.5).astype(int)\n",
    "\n",
    "print(\"Neural network with backpropagation implemented!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Test on XOR Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR dataset\n",
    "X_xor = np.array([[0, 0, 1, 1],\n",
    "                  [0, 1, 0, 1]])\n",
    "Y_xor = np.array([[0, 1, 1, 0]])\n",
    "\n",
    "# Create and train network\n",
    "nn = NeuralNetworkWithBackprop(input_size=2, hidden_size=4, output_size=1, learning_rate=0.5)\n",
    "losses = nn.train(X_xor, Y_xor, num_iterations=2000, print_every=400)\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss (XOR Problem)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Test predictions\n",
    "predictions = nn.predict(X_xor)\n",
    "print(\"\\nXOR Results:\")\n",
    "print(\"Input\\t\\tTarget\\tPrediction\")\n",
    "print(\"-\" * 40)\n",
    "for i in range(4):\n",
    "    print(f\"[{X_xor[0,i]:.0f}, {X_xor[1,i]:.0f}]\\t\\t{Y_xor[0,i]}\\t{predictions[0,i]}\")\n",
    "\n",
    "accuracy = np.mean(predictions == Y_xor) * 100\n",
    "print(f\"\\nAccuracy: {accuracy:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Gradient Checking\n",
    "\n",
    "**Gradient checking** verifies that our backprop implementation is correct by comparing analytical gradients with numerical gradients.\n",
    "\n",
    "### Numerical Gradient:\n",
    "$\\frac{\\partial L}{\\partial \\theta} \\approx \\frac{L(\\theta + \\epsilon) - L(\\theta - \\epsilon)}{2\\epsilon}$\n",
    "\n",
    "If the difference between analytical and numerical gradients is very small (< 1e-7), our implementation is correct!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_check(nn, X, Y, epsilon=1e-7):\n",
    "    \"\"\"\n",
    "    Verify backpropagation implementation using numerical gradients\n",
    "    \"\"\"\n",
    "    # Get analytical gradients from backprop\n",
    "    A2, cache = nn.forward(X)\n",
    "    analytical_grads = nn.backward(Y, cache)\n",
    "    \n",
    "    # Check each parameter\n",
    "    parameters = {'W1': nn.W1, 'b1': nn.b1, 'W2': nn.W2, 'b2': nn.b2}\n",
    "    \n",
    "    print(\"Gradient Checking:\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for name, param in parameters.items():\n",
    "        # Get analytical gradient\n",
    "        analytical = analytical_grads[f'd{name}']\n",
    "        \n",
    "        # Compute numerical gradient for a few random elements\n",
    "        numerical = np.zeros_like(param)\n",
    "        \n",
    "        # Sample a few random indices to check (checking all is slow)\n",
    "        flat_param = param.ravel()\n",
    "        flat_numerical = numerical.ravel()\n",
    "        \n",
    "        num_checks = min(10, len(flat_param))  # check up to 10 elements\n",
    "        indices = np.random.choice(len(flat_param), num_checks, replace=False)\n",
    "        \n",
    "        for idx in indices:\n",
    "            # Compute numerical gradient\n",
    "            old_value = flat_param[idx]\n",
    "            \n",
    "            # L(theta + epsilon)\n",
    "            flat_param[idx] = old_value + epsilon\n",
    "            A2_plus, _ = nn.forward(X)\n",
    "            loss_plus = nn.compute_loss(Y, A2_plus)\n",
    "            \n",
    "            # L(theta - epsilon)\n",
    "            flat_param[idx] = old_value - epsilon\n",
    "            A2_minus, _ = nn.forward(X)\n",
    "            loss_minus = nn.compute_loss(Y, A2_minus)\n",
    "            \n",
    "            # Numerical gradient\n",
    "            flat_numerical[idx] = (loss_plus - loss_minus) / (2 * epsilon)\n",
    "            \n",
    "            # Restore original value\n",
    "            flat_param[idx] = old_value\n",
    "        \n",
    "        # Compute difference\n",
    "        analytical_flat = analytical.ravel()\n",
    "        difference = np.linalg.norm(analytical_flat[indices] - flat_numerical[indices]) / \\\n",
    "                     (np.linalg.norm(analytical_flat[indices]) + np.linalg.norm(flat_numerical[indices]) + 1e-8)\n",
    "        \n",
    "        status = \"✓ PASS\" if difference < 1e-5 else \"✗ FAIL\"\n",
    "        print(f\"{name}: difference = {difference:.2e} {status}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(\"If all checks PASS, backpropagation is implemented correctly!\")\n",
    "\n",
    "# Run gradient check\n",
    "np.random.seed(42)\n",
    "nn_check = NeuralNetworkWithBackprop(2, 4, 1)\n",
    "gradient_check(nn_check, X_xor, Y_xor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. PyTorch Autograd: Automatic Differentiation\n",
    "\n",
    "PyTorch automatically computes gradients for us using **autograd**. No need to implement backprop manually!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Basic Autograd Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors with gradient tracking\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "w = torch.tensor([3.0], requires_grad=True)\n",
    "b = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# Forward pass: y = w*x + b\n",
    "y = w * x + b\n",
    "print(f\"y = w*x + b = {y.item()}\")\n",
    "\n",
    "# Compute gradients automatically\n",
    "y.backward()\n",
    "\n",
    "# Access gradients\n",
    "print(f\"\\nGradients:\")\n",
    "print(f\"  dy/dx = {x.grad.item()} (should be w = {w.item()})\")\n",
    "print(f\"  dy/dw = {w.grad.item()} (should be x = {x.item()})\")\n",
    "print(f\"  dy/db = {b.grad.item()} (should be 1)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Computational Graph in PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: L = (w * x + b)^2\n",
    "x = torch.tensor([3.0], requires_grad=True)\n",
    "w = torch.tensor([2.0], requires_grad=True)\n",
    "b = torch.tensor([1.0], requires_grad=True)\n",
    "\n",
    "# Forward\n",
    "a = w * x\n",
    "z = a + b\n",
    "L = z ** 2\n",
    "\n",
    "print(f\"Forward: L = (w*x + b)^2 = ({w.item()}*{x.item()} + {b.item()})^2 = {L.item()}\")\n",
    "\n",
    "# Backward\n",
    "L.backward()\n",
    "\n",
    "print(f\"\\nGradients computed automatically:\")\n",
    "print(f\"  dL/dw = {w.grad.item()}\")\n",
    "print(f\"  dL/dx = {x.grad.item()}\")\n",
    "print(f\"  dL/db = {b.grad.item()}\")\n",
    "\n",
    "# Verify manually\n",
    "z_val = w.item() * x.item() + b.item()\n",
    "dL_dw_manual = 2 * z_val * x.item()\n",
    "print(f\"\\nManual calculation: dL/dw = 2*z*x = 2*{z_val}*{x.item()} = {dL_dw_manual}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Training a Network with Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XOR with PyTorch autograd\n",
    "X = torch.tensor([[0., 0., 1., 1.],\n",
    "                  [0., 1., 0., 1.]])\n",
    "Y = torch.tensor([[0., 1., 1., 0.]])\n",
    "\n",
    "# Initialize parameters\n",
    "torch.manual_seed(42)\n",
    "W1 = torch.randn(4, 2, requires_grad=True) * 0.1\n",
    "b1 = torch.zeros(4, 1, requires_grad=True)\n",
    "W2 = torch.randn(1, 4, requires_grad=True) * 0.1\n",
    "b2 = torch.zeros(1, 1, requires_grad=True)\n",
    "\n",
    "learning_rate = 0.5\n",
    "losses = []\n",
    "\n",
    "for i in range(2000):\n",
    "    # Forward pass\n",
    "    Z1 = W1 @ X + b1\n",
    "    A1 = torch.relu(Z1)\n",
    "    Z2 = W2 @ A1 + b2\n",
    "    A2 = torch.sigmoid(Z2)\n",
    "    \n",
    "    # Loss\n",
    "    loss = torch.mean((Y - A2)**2)\n",
    "    losses.append(loss.item())\n",
    "    \n",
    "    # Backward pass (automatic!)\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update parameters\n",
    "    with torch.no_grad():\n",
    "        W1 -= learning_rate * W1.grad\n",
    "        b1 -= learning_rate * b1.grad\n",
    "        W2 -= learning_rate * W2.grad\n",
    "        b2 -= learning_rate * b2.grad\n",
    "        \n",
    "        # Zero gradients\n",
    "        W1.grad.zero_()\n",
    "        b1.grad.zero_()\n",
    "        W2.grad.zero_()\n",
    "        b2.grad.zero_()\n",
    "    \n",
    "    if (i + 1) % 400 == 0:\n",
    "        print(f\"Iteration {i+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training with PyTorch Autograd')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Test\n",
    "with torch.no_grad():\n",
    "    Z1 = W1 @ X + b1\n",
    "    A1 = torch.relu(Z1)\n",
    "    Z2 = W2 @ A1 + b2\n",
    "    A2 = torch.sigmoid(Z2)\n",
    "    predictions = (A2 > 0.5).float()\n",
    "\n",
    "print(\"\\nPredictions:\")\n",
    "print(predictions)\n",
    "print(\"Targets:\")\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Understanding Gradient Flow\n",
    "\n",
    "Let's visualize how gradients flow backward through the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple network and track gradients at each layer\n",
    "class GradientTrackingNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(2, 8)\n",
    "        self.fc2 = nn.Linear(8, 4)\n",
    "        self.fc3 = nn.Linear(4, 1)\n",
    "        self.activations = []\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.activations = []\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        self.activations.append(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = torch.relu(x)\n",
    "        self.activations.append(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        self.activations.append(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Train briefly\n",
    "model = GradientTrackingNet()\n",
    "X_torch = torch.tensor([[0., 0., 1., 1.],\n",
    "                        [0., 1., 0., 1.]], dtype=torch.float32).T\n",
    "Y_torch = torch.tensor([[0., 1., 1., 0.]], dtype=torch.float32).T\n",
    "\n",
    "# Single forward-backward pass\n",
    "output = model(X_torch)\n",
    "loss = nn.BCELoss()(output, Y_torch)\n",
    "loss.backward()\n",
    "\n",
    "# Examine gradients\n",
    "print(\"Gradient magnitudes at each layer:\")\n",
    "print(f\"Layer 1 (W): {model.fc1.weight.grad.abs().mean().item():.6f}\")\n",
    "print(f\"Layer 2 (W): {model.fc2.weight.grad.abs().mean().item():.6f}\")\n",
    "print(f\"Layer 3 (W): {model.fc3.weight.grad.abs().mean().item():.6f}\")\n",
    "print(\"\\nNotice: Gradients often decrease in earlier layers (vanishing gradient problem)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "1. **Backpropagation = Chain Rule**\n",
    "   - Systematically apply chain rule to compute gradients\n",
    "   - Work backward from loss to inputs\n",
    "   - Reuse intermediate computations (efficient!)\n",
    "\n",
    "2. **Two-Phase Algorithm**:\n",
    "   - **Forward pass**: Compute activations, store values\n",
    "   - **Backward pass**: Compute gradients using chain rule\n",
    "\n",
    "3. **Implementation Details**:\n",
    "   - Cache all intermediate values during forward pass\n",
    "   - For each layer, compute: $dZ^{[l]}, dW^{[l]}, db^{[l]}$\n",
    "   - Propagate gradient backwards: $dA^{[l-1]} = (W^{[l]})^T dZ^{[l]}$\n",
    "\n",
    "4. **Gradient Checking**:\n",
    "   - Verify backprop with numerical gradients\n",
    "   - Should match to high precision (~1e-7)\n",
    "   - Use only for debugging (slow!)\n",
    "\n",
    "5. **PyTorch Autograd**:\n",
    "   - Automatically builds computational graph\n",
    "   - `.backward()` computes all gradients\n",
    "   - No need to implement backprop manually!\n",
    "\n",
    "### The Big Picture:\n",
    "\n",
    "Backpropagation makes deep learning possible by efficiently computing gradients in deep networks. Without it, training would be prohibitively slow!\n",
    "\n",
    "### Next Steps:\n",
    "In the next notebook, we'll explore **optimization algorithms** - how to use these gradients to effectively train networks!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Practice Exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 1: Implement backprop for a 3-layer network\n",
    "# Extend NeuralNetworkWithBackprop to support 3 layers\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2: Compute gradients manually for this function\n",
    "# L = (w1*x1 + w2*x2)^2 + (w1*x1 - w2*x2)^2\n",
    "# Find dL/dw1 and dL/dw2\n",
    "# Then verify with PyTorch autograd\n",
    "# Your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 3: Implement gradient checking for all parameters\n",
    "# Modify gradient_check to check ALL elements, not just a sample\n",
    "# Your code here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
