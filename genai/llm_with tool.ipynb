{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a806f3b",
   "metadata": {},
   "source": [
    "# LLM with tool\n",
    "\n",
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f249114",
   "metadata": {},
   "source": [
    "\"You don't need to know how a car works to drive it.\" In this lesson, we'll just focus on how to use LLMs with a single tool."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a678af",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Setup begins with importing the packages (\"tools\") we need to initialize a model. We'll start with Anthropic and OpenAI models. We'll use LangChain, which provide model agnostic tools for playing with these models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a52cebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Get model agnostic tool for initializing models\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "# Quick start for LangGraph\n",
    "from langgraph.prebuilt import create_react_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c75ff7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangSmith API Key: ✓ Set\n",
      "Tracing enabled: ✓ Yes\n",
      "OpenAI API Key: ✓ Set\n"
     ]
    }
   ],
   "source": [
    "# Load .env file (this will automatically set all environment variables)\n",
    "load_dotenv()\n",
    "\n",
    "# Verify LangSmith is configured\n",
    "print(f\"LangSmith API Key: {'✓ Set' if os.environ.get('LANGSMITH_API_KEY') else '✗ Not set'}\")\n",
    "print(f\"Tracing enabled: {'✓ Yes' if os.environ.get('LANGSMITH_TRACING') == 'true' else '✗ No'}\")\n",
    "print(f\"OpenAI API Key: {'✓ Set' if os.environ.get('OPENAI_API_KEY') else '✗ Not set'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a32dea",
   "metadata": {},
   "source": [
    "### Intiialize models\n",
    "\n",
    "Below we use LangChain's init_chat_model() function, which just needs the model name and provider. In this case, we just pass both into the \"model\" argument.\n",
    "\n",
    "Once \"initialized\", we can \"invoke\" the models by passing it input text to generate output text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d11c33c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to be asking a simple math question\n",
    "question = \"What is 2*5?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4ee85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "claude_sonnet = init_chat_model(\n",
    "    model=\"anthropic:claude-3-5-sonnet-latest\",\n",
    "    temperature=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd7ed3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude response:\n",
      "\n",
      "('content', '2*5 = 10')\n",
      "('additional_kwargs', {})\n",
      "('response_metadata', {'id': 'msg_01NTawVyjnzg1L6PU9g33ULD', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 14, 'output_tokens': 11, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-5-sonnet-20241022'})\n",
      "('type', 'ai')\n",
      "('name', None)\n",
      "('id', 'run--780975d6-0c7c-4ad2-af62-46c2326a6044-0')\n",
      "('example', False)\n",
      "('tool_calls', [])\n",
      "('invalid_tool_calls', [])\n",
      "('usage_metadata', {'input_tokens': 14, 'output_tokens': 11, 'total_tokens': 25, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Invoke the models \n",
    "response_claude = claude_sonnet.invoke(question)\n",
    "\n",
    "# In LangChain, we get a lot of output (not just the response). Let's iterate over the response to see.\n",
    "print(\"Claude response:\\n\")\n",
    "for output in response_claude:\n",
    "    print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7ed989b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenAI response:\n",
      "\n",
      "('content', '2 multiplied by 5 equals 10.')\n",
      "('additional_kwargs', {'refusal': None})\n",
      "('response_metadata', {'token_usage': {'completion_tokens': 86, 'prompt_tokens': 13, 'total_tokens': 99, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 64, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'o3-mini-2025-01-31', 'system_fingerprint': 'fp_6c43dcef8c', 'id': 'chatcmpl-CKvCvwqTJXReG7hRb7bWIwpO0hH04', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None})\n",
      "('type', 'ai')\n",
      "('name', None)\n",
      "('id', 'run--5e5805cf-65ad-497c-b85a-9206f4b702c7-0')\n",
      "('example', False)\n",
      "('tool_calls', [])\n",
      "('invalid_tool_calls', [])\n",
      "('usage_metadata', {'input_tokens': 13, 'output_tokens': 86, 'total_tokens': 99, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 64}})\n"
     ]
    }
   ],
   "source": [
    "# Initialize models\n",
    "o3_mini = init_chat_model(\n",
    "    model=\"openai:o3-mini\"\n",
    ")\n",
    "\n",
    "# Invoke the models \n",
    "response_openai = o3_mini.invoke(question)\n",
    "\n",
    "# In LangChain, we get a lot of output (not just the response). Let's iterate over the response to see.\n",
    "print(\"OpenAI response:\\n\")\n",
    "for output in response_openai:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b444f067",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "To learn more about this output, you can read more about what these mean in [LangChain's documentation](https://python.langchain.com/api_reference/langchain/chat_models/langchain.chat_models.base.init_chat_model.html#langchain.chat_models.base.init_chat_model)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95eb165f",
   "metadata": {},
   "source": [
    "### Tools\n",
    "Both models got the answer correct, but this is not always the case with math problems, for which reason, we may want to \"bind\" the LLM to a math tool. Let's create a multiplication and division tool functions that the LLMs can call.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95b2be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a, b):\n",
    "    \"\"\"\n",
    "    This tool multiplies two numbers and returns the result.\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "def divide(a, b):\n",
    "    \"\"\"\n",
    "    This tool divides two numbers and returns the result.\n",
    "    \"\"\"\n",
    "    return a / b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dbaff362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claude response:\n",
      "\n",
      "('content', [{'text': \"I'll help you multiply 2 and 5 using the multiply tool.\", 'type': 'text'}, {'id': 'toolu_011fxEj5mjp7pi9k5hzRPHhH', 'input': {'a': 2, 'b': 5}, 'name': 'multiply', 'type': 'tool_use'}])\n",
      "('additional_kwargs', {})\n",
      "('response_metadata', {'id': 'msg_013DeUgVxS1HHzhYR5PYq5BY', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 445, 'output_tokens': 86, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-5-sonnet-20241022'})\n",
      "('type', 'ai')\n",
      "('name', None)\n",
      "('id', 'run--906ccbad-7dd0-4f8f-8e56-3073ccbfe270-0')\n",
      "('example', False)\n",
      "('tool_calls', [{'name': 'multiply', 'args': {'a': 2, 'b': 5}, 'id': 'toolu_011fxEj5mjp7pi9k5hzRPHhH', 'type': 'tool_call'}])\n",
      "('invalid_tool_calls', [])\n",
      "('usage_metadata', {'input_tokens': 445, 'output_tokens': 86, 'total_tokens': 531, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}})\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now let's bind the tool to the model (using Anthropic only)\n",
    "claude_sonnet_with_tool = claude_sonnet.bind_tools([multiply, divide])\n",
    "\n",
    "# Now let's invoke the model with the tool\n",
    "response_claude_with_tool = claude_sonnet_with_tool.invoke(question)\n",
    "\n",
    "# In LangChain, we get a lot of output (not just the response). Let's iterate over the response to see.\n",
    "print(\"Claude response:\\n\")\n",
    "for output in response_claude_with_tool:\n",
    "    print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b472f7da",
   "metadata": {},
   "source": [
    "### Observation\n",
    "Notice how the LLM decides to use the tool, \"multiply\", and provides the tool with the inputs (2 and 5), which are set to \"a\" and \"b\". But we don't get an answer. This is because tool calling requires another LLM call to incorporate the response of the tool call. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c415c99b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': \"I'll help you multiply 2 and 5 using the multiply tool.\",\n",
       "  'type': 'text'},\n",
       " {'id': 'toolu_011fxEj5mjp7pi9k5hzRPHhH',\n",
       "  'input': {'a': 2, 'b': 5},\n",
       "  'name': 'multiply',\n",
       "  'type': 'tool_use'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's unpack the response_metadata's \"content\" more clearly to see the result.\n",
    "response_claude_with_tool.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "40c0428e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of tool calls: 1\n",
      "Tool called: multiply\n",
      "Arguments: {'a': 2, 'b': 5}\n"
     ]
    }
   ],
   "source": [
    "# To get the actual result, we need to execute the tool call\n",
    "# First, let's see what tool was called. \n",
    "# The [0] is because we assume there is only one tool call (0 is the first index)\n",
    "tool_call = response_claude_with_tool.tool_calls[0]\n",
    "print(f\"# of tool calls: {len(response_claude_with_tool.tool_calls)}\")\n",
    "print(f\"Tool called: {tool_call['name']}\")\n",
    "print(f\"Arguments: {tool_call['args']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16ed0b51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Result: 10\n"
     ]
    }
   ],
   "source": [
    "# Because \"Arguments\" contains the inputs to the tool, we can use it to execute the tool\n",
    "result = multiply(**tool_call['args'])\n",
    "\n",
    "# The result, 10, is then incorporated into the original response on the subsequent LLM call\n",
    "print(f\"Result: {result}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2a81fe",
   "metadata": {},
   "source": [
    "### LangGraph\n",
    "\n",
    "If we are to put this altogether, we would need a chat bot that can keep the LLM calls going until the LLM can confidently end the response with an answer. To do this, we'll use LangGraph, which sets LLMs and tools as nodes and their relationships as edges. \n",
    "\n",
    "In our case, we have 1 LLM and 1 tool, so we just have to set up the following nodes:\n",
    "- START\n",
    "- LLM\n",
    "- TOOL\n",
    "- END\n",
    "\n",
    "And the following edges:\n",
    "- START --> LLM\n",
    "- LLM --> TOOL (conditional)\n",
    "- TOOL --> LLM\n",
    "- LLM --> END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d2019e70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: 3 + 4 = 7\n",
      "\n",
      "This call has been traced to LangSmith!\n",
      "Check your LangSmith dashboard to see the trace.\n"
     ]
    }
   ],
   "source": [
    "# Now all your model calls will be automatically traced in LangSmith!\n",
    "# You can view the traces at: https://smith.langchain.com\n",
    "\n",
    "# Let's test it with a simple call\n",
    "test_response = claude_sonnet.invoke(\"What is 3 + 4?\")\n",
    "\n",
    "print(\"Response:\", test_response.content)\n",
    "print(\"\\nThis call has been traced to LangSmith!\")\n",
    "print(\"Check your LangSmith dashboard to see the trace.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc085f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are going to use create_react_agent to use LangGraph's prebuilt agent framework. \n",
    "# This saves us the trouble of setting up the nodes and edges manually.\n",
    "agent = create_react_agent(\n",
    "    model=\"anthropic:claude-3-5-sonnet-latest\",\n",
    "    tools=[multiply, divide],\n",
    "    prompt=\"You are a helpful assistant\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5595ed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the agent\n",
    "agent_response = agent.invoke(\n",
    "    {\"messages\": [{\"role\": \"user\", \"content\": question}]}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "accb2251",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='What is 2*5?', additional_kwargs={}, response_metadata={}, id='0abf7e65-fe69-4902-8642-081cb4d3ae82'),\n",
       "  AIMessage(content=[{'text': \"I'll help you multiply 2 and 5 using the multiply tool.\", 'type': 'text'}, {'id': 'toolu_01TRYXEFqMnaJAF6Ki3woHaW', 'input': {'a': 2, 'b': 5}, 'name': 'multiply', 'type': 'tool_use'}], additional_kwargs={}, response_metadata={'id': 'msg_01V7kmPch6bfPydRK2YgMhBY', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'tool_use', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 451, 'output_tokens': 86, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-5-sonnet-20241022'}, id='run--04819116-d885-4d63-b70c-357a83a5010d-0', tool_calls=[{'name': 'multiply', 'args': {'a': 2, 'b': 5}, 'id': 'toolu_01TRYXEFqMnaJAF6Ki3woHaW', 'type': 'tool_call'}], usage_metadata={'input_tokens': 451, 'output_tokens': 86, 'total_tokens': 537, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}}),\n",
       "  ToolMessage(content='10', name='multiply', id='40a2289d-7bea-4423-8048-0081b488c502', tool_call_id='toolu_01TRYXEFqMnaJAF6Ki3woHaW'),\n",
       "  AIMessage(content='2 multiplied by 5 equals 10.', additional_kwargs={}, response_metadata={'id': 'msg_01X7jGWcu8zTQFXty1QogKCh', 'model': 'claude-3-5-sonnet-20241022', 'stop_reason': 'end_turn', 'stop_sequence': None, 'usage': {'cache_creation': {'ephemeral_1h_input_tokens': 0, 'ephemeral_5m_input_tokens': 0}, 'cache_creation_input_tokens': 0, 'cache_read_input_tokens': 0, 'input_tokens': 549, 'output_tokens': 16, 'server_tool_use': None, 'service_tier': 'standard'}, 'model_name': 'claude-3-5-sonnet-20241022'}, id='run--ebb53f3e-ff90-413b-96d4-106f35555f49-0', usage_metadata={'input_tokens': 549, 'output_tokens': 16, 'total_tokens': 565, 'input_token_details': {'cache_read': 0, 'cache_creation': 0, 'ephemeral_5m_input_tokens': 0, 'ephemeral_1h_input_tokens': 0}})]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is the full response\n",
    "agent_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e09cd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human :  What is 2*5?\n",
      "ai :  [{'text': \"I'll help you multiply 2 and 5 using the multiply tool.\", 'type': 'text'}, {'id': 'toolu_01TRYXEFqMnaJAF6Ki3woHaW', 'input': {'a': 2, 'b': 5}, 'name': 'multiply', 'type': 'tool_use'}]\n",
      "tool :  10\n",
      "ai :  2 multiplied by 5 equals 10.\n"
     ]
    }
   ],
   "source": [
    "# Let's take a look at the messages\n",
    "for message in agent_response['messages']:\n",
    "    print(message.type, \": \", message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b9ceaf",
   "metadata": {},
   "source": [
    "# That is it\n",
    "\n",
    "Now you can create your own chat bot! Check out LangSmith for observability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
